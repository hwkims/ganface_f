{"cells":[{"cell_type":"code","execution_count":null,"metadata":{"id":"Ee4TTCN-fVMa"},"outputs":[],"source":["# Google Drive 마운트\n","from google.colab import drive\n","drive.mount('/content/drive')\n","!pip install torch torchvision matplotlib numpy pillow\n","import os\n","from torchvision import datasets, transforms\n","from torch.utils.data import DataLoader\n","\n","# Google Drive에서 이미지 경로 설정\n","image_folder_path = '/content/drive/MyDrive/GANFACE/filtered_images'\n","\n","# 이미지 전처리 설정 (Resize, Normalize, ToTensor 등)\n","transform = transforms.Compose([\n","    transforms.Resize(256),  # 256x256 크기로 리사이즈\n","    transforms.ToTensor(),   # 텐서로 변환\n","    transforms.Normalize((0.5,), (0.5,)),  # 정규화 (0.5, 0.5)로 예시\n","])\n","\n","# ImageFolder를 사용하여 데이터셋 로드\n","train_dataset = datasets.ImageFolder(root=image_folder_path, transform=transform)\n","\n","# DataLoader 설정 (배치 사이즈 16)\n","train_dataloader = DataLoader(train_dataset, batch_size=16, shuffle=True)\n","import torch\n","import torch.nn as nn\n","import torch.nn.functional as F\n","\n","# Self-Attention Layer\n","class SelfAttention(nn.Module):\n","    def __init__(self, in_channels):\n","        super(SelfAttention, self).__init__()\n","\n","        self.query_conv = nn.Conv2d(in_channels, in_channels // 8, kernel_size=1)\n","        self.key_conv = nn.Conv2d(in_channels, in_channels // 8, kernel_size=1)\n","        self.value_conv = nn.Conv2d(in_channels, in_channels, kernel_size=1)\n","        self.gamma = nn.Parameter(torch.zeros(1))\n","\n","    def forward(self, x):\n","        batch_size, channels, height, width = x.size()\n","\n","        # Compute query, key, value\n","        query = self.query_conv(x).view(batch_size, -1, height * width)\n","        key = self.key_conv(x).view(batch_size, -1, height * width)\n","        value = self.value_conv(x).view(batch_size, -1, height * width)\n","\n","        # Scaled dot-product attention\n","        attention = torch.bmm(query.permute(0, 2, 1), key)  # Q*K^T\n","        attention = F.softmax(attention, dim=-1)\n","\n","        out = torch.bmm(value, attention.permute(0, 2, 1))  # V*Attention\n","        out = out.view(batch_size, channels, height, width)\n","\n","        return self.gamma * out + x  # Apply attention and residual connection\n","\n","# Spatial Attention Layer\n","class SpatialAttention(nn.Module):\n","    def __init__(self, in_channels):\n","        super(SpatialAttention, self).__init__()\n","        self.conv1 = nn.Conv2d(in_channels, 1, kernel_size=7, stride=1, padding=3)\n","\n","    def forward(self, x):\n","        attention_map = torch.sigmoid(self.conv1(x))  # Apply sigmoid to create attention map\n","        return x * attention_map\n","# Generator 모델 정의\n","class GeneratorWithAttention(nn.Module):\n","    def __init__(self, input_channels, output_channels):\n","        super(GeneratorWithAttention, self).__init__()\n","\n","        # Encoder\n","        self.encoder = nn.Sequential(\n","            nn.Conv2d(input_channels, 64, kernel_size=4, stride=2, padding=1),\n","            nn.ReLU(),\n","            nn.Conv2d(64, 128, kernel_size=4, stride=2, padding=1),\n","            nn.ReLU(),\n","            nn.Conv2d(128, 256, kernel_size=4, stride=2, padding=1),\n","            nn.ReLU(),\n","            SelfAttention(256),  # Self-Attention 추가\n","            nn.Conv2d(256, 512, kernel_size=4, stride=2, padding=1),\n","            nn.ReLU(),\n","            SpatialAttention(512),  # Spatial Attention 추가\n","        )\n","\n","        # Decoder\n","        self.decoder = nn.Sequential(\n","            nn.ConvTranspose2d(512, 256, kernel_size=4, stride=2, padding=1),\n","            nn.ReLU(),\n","            nn.ConvTranspose2d(256, 128, kernel_size=4, stride=2, padding=1),\n","            nn.ReLU(),\n","            nn.ConvTranspose2d(128, 64, kernel_size=4, stride=2, padding=1),\n","            nn.ReLU(),\n","            nn.ConvTranspose2d(64, output_channels, kernel_size=4, stride=2, padding=1),\n","            nn.Tanh(),\n","        )\n","\n","    def forward(self, x):\n","        x = self.encoder(x)\n","        x = self.decoder(x)\n","        return x\n","\n","# Discriminator 모델 정의\n","class Discriminator(nn.Module):\n","    def __init__(self, input_channels):\n","        super(Discriminator, self).__init__()\n","\n","        self.model = nn.Sequential(\n","            nn.Conv2d(input_channels, 64, kernel_size=4, stride=2, padding=1),\n","            nn.LeakyReLU(0.2),\n","            nn.Conv2d(64, 128, kernel_size=4, stride=2, padding=1),\n","            nn.LeakyReLU(0.2),\n","            nn.Conv2d(128, 256, kernel_size=4, stride=2, padding=1),\n","            nn.LeakyReLU(0.2),\n","            nn.Conv2d(256, 512, kernel_size=4, stride=2, padding=1),\n","            nn.LeakyReLU(0.2),\n","            nn.Conv2d(512, 1, kernel_size=4, stride=1, padding=1),\n","            nn.Sigmoid(),\n","        )\n","\n","    def forward(self, x):\n","        return self.model(x)\n","# Adversarial Loss (BCE)\n","def adversarial_loss(disc_pred, target):\n","    return torch.mean((disc_pred - target) ** 2)\n","\n","# Cycle Consistency Loss\n","def cycle_consistency_loss(real_image, reconstructed_image):\n","    return torch.mean((real_image - reconstructed_image) ** 2)\n","\n","# 훈련 루프\n","def train(cyclegan, dataloader, num_epochs, device):\n","    optimizer_g = torch.optim.Adam(cyclegan.parameters(), lr=0.0002, betas=(0.5, 0.999))\n","    optimizer_d = torch.optim.Adam(cyclegan.parameters(), lr=0.0002, betas=(0.5, 0.999))\n","\n","    for epoch in range(num_epochs):\n","        for i, data in enumerate(dataloader):\n","            real_A, real_B = data\n","            real_A, real_B = real_A.to(device), real_B.to(device)\n","\n","            # Generator A->B 업데이트 (스케치 -> 얼굴 사진)\n","            optimizer_g.zero_grad()\n","            fake_B = cyclegan.generator_AtoB(real_A)\n","            loss_G_AtoB = adversarial_loss(cyclegan.discriminator_B(fake_B), torch.ones_like(fake_B)) \\\n","                          + cycle_consistency_loss(real_A, cyclegan.generator_BtoA(fake_B))\n","            loss_G_AtoB.backward()\n","            optimizer_g.step()\n","\n","            # Generator B->A 업데이트 (얼굴 사진 -> 스케치)\n","            optimizer_g.zero_grad()\n","            fake_A = cyclegan.generator_BtoA(real_B)\n","            loss_G_BtoA = adversarial_loss(cyclegan.discriminator_A(fake_A), torch.ones_like(fake_A)) \\\n","                          + cycle_consistency_loss(real_B, cyclegan.generator_AtoB(fake_A))\n","            loss_G_BtoA.backward()\n","            optimizer_g.step()\n","\n","            # Discriminator A 업데이트\n","            optimizer_d.zero_grad()\n","            loss_D_A = adversarial_loss(cyclegan.discriminator_A(real_A), torch.ones_like(real_A)) \\\n","                       + adversarial_loss(cyclegan.discriminator_A(fake_A.detach()), torch.zeros_like(fake_A))\n","            loss_D_A.backward()\n","            optimizer_d.step()\n","\n","            # Discriminator B 업데이트\n","            optimizer_d.zero_grad()\n","            loss_D_B = adversarial_loss(cyclegan.discriminator_B(real_B), torch.ones_like(real_B)) \\\n","                       + adversarial_loss(cyclegan.discriminator_B(fake_B.detach()), torch.zeros_like(fake_B))\n","            loss_D_B.backward()\n","            optimizer_d.step()\n","\n","            if i % 100 == 0:\n","                print(f\"[Epoch {epoch}/{num_epochs}] [Batch {i}/{len(dataloader)}] \"\n","                      f\"[D loss: {loss_D_A.item() + loss_D_B.item()}] \"\n","                      f\"[G loss: {loss_G_AtoB.item() + loss_G_BtoA.item()}]\")\n","\n","        # 중간 결과 저장\n","        if epoch % 10 == 0:\n","            save_image(fake_B.data[:25], f\"generated_images_epoch_{epoch}.png\", nrow=5, normalize=True)\n","device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n","cyclegan = CycleGAN().to(device)  # CycleGAN 모델 인스턴스화\n","train(cyclegan, train_dataloader, num_epochs=200, device=device)\n"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":723},"executionInfo":{"elapsed":46242,"status":"error","timestamp":1733884008388,"user":{"displayName":"현우","userId":"14235240467528686656"},"user_tz":-540},"id":"i1VnIAOAgpBg","outputId":"ce0298a4-8c98-4c62-e301-3a10680f8f49"},"outputs":[{"name":"stdout","output_type":"stream","text":["Mounted at /content/drive\n","Requirement already satisfied: torch in /usr/local/lib/python3.10/dist-packages (2.5.1+cu121)\n","Requirement already satisfied: torchvision in /usr/local/lib/python3.10/dist-packages (0.20.1+cu121)\n","Requirement already satisfied: matplotlib in /usr/local/lib/python3.10/dist-packages (3.8.0)\n","Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (1.26.4)\n","Requirement already satisfied: pillow in /usr/local/lib/python3.10/dist-packages (11.0.0)\n","Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch) (3.16.1)\n","Requirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.10/dist-packages (from torch) (4.12.2)\n","Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch) (3.4.2)\n","Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch) (3.1.4)\n","Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch) (2024.10.0)\n","Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.10/dist-packages (from torch) (1.13.1)\n","Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy==1.13.1->torch) (1.3.0)\n","Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (1.3.1)\n","Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (0.12.1)\n","Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (4.55.1)\n","Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (1.4.7)\n","Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (24.2)\n","Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (3.2.0)\n","Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (2.8.2)\n","Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.7->matplotlib) (1.16.0)\n","Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch) (3.0.2)\n"]},{"ename":"RuntimeError","evalue":"shape '[16, 1, 15, 15]' is invalid for input of size 3145728","output_type":"error","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)","\u001b[0;32m<ipython-input-1-663742e38b6e>\u001b[0m in \u001b[0;36m<cell line: 217>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    215\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    216\u001b[0m \u001b[0;31m# 훈련 시작\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 217\u001b[0;31m \u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcyclegan\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_dataloader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_epochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m200\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m","\u001b[0;32m<ipython-input-1-663742e38b6e>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(cyclegan, dataloader, num_epochs, device)\u001b[0m\n\u001b[1;32m    176\u001b[0m             \u001b[0moptimizer_g\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    177\u001b[0m             \u001b[0mfake_B\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcyclegan\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgenerator_AtoB\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mreal_A\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 178\u001b[0;31m             \u001b[0mloss_G_AtoB\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0madversarial_loss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcyclegan\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdiscriminator_B\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfake_B\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mones_like\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfake_B\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m\\\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    179\u001b[0m                           \u001b[0;34m+\u001b[0m \u001b[0mcycle_consistency_loss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mreal_A\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcyclegan\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgenerator_BtoA\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfake_B\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    180\u001b[0m             \u001b[0mloss_G_AtoB\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m<ipython-input-1-663742e38b6e>\u001b[0m in \u001b[0;36madversarial_loss\u001b[0;34m(disc_pred, target)\u001b[0m\n\u001b[1;32m    157\u001b[0m \u001b[0;31m# 수정된 Loss 함수\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    158\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0madversarial_loss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdisc_pred\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 159\u001b[0;31m     \u001b[0mtarget\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtarget\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mview_as\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdisc_pred\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# target 크기 맞추기\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    160\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmean\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdisc_pred\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mtarget\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m**\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    161\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mRuntimeError\u001b[0m: shape '[16, 1, 15, 15]' is invalid for input of size 3145728"]}],"source":["# Google Drive 마운트\n","from google.colab import drive\n","drive.mount('/content/drive')\n","\n","# 필요한 라이브러리 설치\n","!pip install torch torchvision matplotlib numpy pillow\n","\n","import os\n","import torch\n","import torch.nn as nn\n","import torch.nn.functional as F\n","from torchvision import transforms\n","from torch.utils.data import DataLoader, Dataset\n","from PIL import Image\n","import matplotlib.pyplot as plt\n","from torchvision.utils import save_image\n","\n","# Google Drive에서 이미지 경로 설정\n","image_folder_path = '/content/drive/MyDrive/GANFACE/filtered_images'\n","\n","# 이미지 전처리 설정 (Resize, Normalize, ToTensor 등)\n","transform = transforms.Compose([\n","    transforms.Resize(256),  # 256x256 크기로 리사이즈\n","    transforms.ToTensor(),   # 텐서로 변환\n","    transforms.Normalize((0.5,), (0.5,)),  # 정규화 (0.5, 0.5)로 예시\n","])\n","\n","# 사용자 정의 Dataset 클래스\n","class CustomImageDataset(Dataset):\n","    def __init__(self, image_folder_path, transform=None):\n","        self.image_folder_path = image_folder_path\n","        self.transform = transform\n","        self.image_paths = [os.path.join(image_folder_path, fname) for fname in os.listdir(image_folder_path) if fname.endswith('.png') or fname.endswith('.jpg')]\n","\n","    def __len__(self):\n","        return len(self.image_paths)\n","\n","    def __getitem__(self, idx):\n","        image_path = self.image_paths[idx]\n","        image = Image.open(image_path).convert('RGB')  # 이미지 열기 (RGB로 변환)\n","\n","        if self.transform:\n","            image = self.transform(image)  # 전처리\n","\n","        return image\n","\n","# CustomDataset을 사용하여 데이터셋 로드\n","train_dataset = CustomImageDataset(image_folder_path=image_folder_path, transform=transform)\n","\n","# DataLoader 설정 (배치 사이즈 16)\n","train_dataloader = DataLoader(train_dataset, batch_size=16, shuffle=True)\n","\n","# Self-Attention Layer\n","class SelfAttention(nn.Module):\n","    def __init__(self, in_channels):\n","        super(SelfAttention, self).__init__()\n","\n","        self.query_conv = nn.Conv2d(in_channels, in_channels // 8, kernel_size=1)\n","        self.key_conv = nn.Conv2d(in_channels, in_channels // 8, kernel_size=1)\n","        self.value_conv = nn.Conv2d(in_channels, in_channels, kernel_size=1)\n","        self.gamma = nn.Parameter(torch.zeros(1))\n","\n","    def forward(self, x):\n","        batch_size, channels, height, width = x.size()\n","\n","        query = self.query_conv(x).view(batch_size, -1, height * width)\n","        key = self.key_conv(x).view(batch_size, -1, height * width)\n","        value = self.value_conv(x).view(batch_size, -1, height * width)\n","\n","        attention = torch.bmm(query.permute(0, 2, 1), key)\n","        attention = F.softmax(attention, dim=-1)\n","\n","        out = torch.bmm(value, attention.permute(0, 2, 1))\n","        out = out.view(batch_size, channels, height, width)\n","\n","        return self.gamma * out + x\n","\n","# Spatial Attention Layer\n","class SpatialAttention(nn.Module):\n","    def __init__(self, in_channels):\n","        super(SpatialAttention, self).__init__()\n","        self.conv1 = nn.Conv2d(in_channels, 1, kernel_size=7, stride=1, padding=3)\n","\n","    def forward(self, x):\n","        attention_map = torch.sigmoid(self.conv1(x))\n","        return x * attention_map\n","\n","# Generator 모델 정의\n","class GeneratorWithAttention(nn.Module):\n","    def __init__(self, input_channels, output_channels):\n","        super(GeneratorWithAttention, self).__init__()\n","\n","        # Encoder\n","        self.encoder = nn.Sequential(\n","            nn.Conv2d(input_channels, 64, kernel_size=4, stride=2, padding=1),\n","            nn.ReLU(),\n","            nn.Conv2d(64, 128, kernel_size=4, stride=2, padding=1),\n","            nn.ReLU(),\n","            nn.Conv2d(128, 256, kernel_size=4, stride=2, padding=1),\n","            nn.ReLU(),\n","            SelfAttention(256),\n","            nn.Conv2d(256, 512, kernel_size=4, stride=2, padding=1),\n","            nn.ReLU(),\n","            SpatialAttention(512),\n","        )\n","\n","        # Decoder\n","        self.decoder = nn.Sequential(\n","            nn.ConvTranspose2d(512, 256, kernel_size=4, stride=2, padding=1),\n","            nn.ReLU(),\n","            nn.ConvTranspose2d(256, 128, kernel_size=4, stride=2, padding=1),\n","            nn.ReLU(),\n","            nn.ConvTranspose2d(128, 64, kernel_size=4, stride=2, padding=1),\n","            nn.ReLU(),\n","            nn.ConvTranspose2d(64, output_channels, kernel_size=4, stride=2, padding=1),\n","            nn.Tanh(),\n","        )\n","\n","    def forward(self, x):\n","        x = self.encoder(x)\n","        x = self.decoder(x)\n","        return x\n","\n","# Discriminator 모델 정의\n","class Discriminator(nn.Module):\n","    def __init__(self, input_channels):\n","        super(Discriminator, self).__init__()\n","\n","        self.model = nn.Sequential(\n","            nn.Conv2d(input_channels, 64, kernel_size=4, stride=2, padding=1),\n","            nn.LeakyReLU(0.2),\n","            nn.Conv2d(64, 128, kernel_size=4, stride=2, padding=1),\n","            nn.LeakyReLU(0.2),\n","            nn.Conv2d(128, 256, kernel_size=4, stride=2, padding=1),\n","            nn.LeakyReLU(0.2),\n","            nn.Conv2d(256, 512, kernel_size=4, stride=2, padding=1),\n","            nn.LeakyReLU(0.2),\n","            nn.Conv2d(512, 1, kernel_size=4, stride=1, padding=1),\n","            nn.Sigmoid(),\n","        )\n","\n","    def forward(self, x):\n","        return self.model(x)\n","\n","# CycleGAN 모델 정의\n","class CycleGAN(nn.Module):\n","    def __init__(self, input_channels, output_channels):\n","        super(CycleGAN, self).__init__()\n","        self.generator_AtoB = GeneratorWithAttention(input_channels, output_channels)\n","        self.generator_BtoA = GeneratorWithAttention(output_channels, input_channels)\n","        self.discriminator_A = Discriminator(input_channels)\n","        self.discriminator_B = Discriminator(output_channels)\n","\n","    def forward(self, x):\n","        pass  # forward()는 실제로 필요없음\n","\n","# 수정된 Loss 함수\n","def adversarial_loss(disc_pred, target):\n","    target = target.view_as(disc_pred)  # target 크기 맞추기\n","    return torch.mean((disc_pred - target) ** 2)\n","\n","def cycle_consistency_loss(real_image, reconstructed_image):\n","    return torch.mean((real_image - reconstructed_image) ** 2)\n","\n","# 훈련 루프\n","def train(cyclegan, dataloader, num_epochs, device):\n","    optimizer_g = torch.optim.Adam(cyclegan.parameters(), lr=0.0002, betas=(0.5, 0.999))\n","    optimizer_d = torch.optim.Adam(cyclegan.parameters(), lr=0.0002, betas=(0.5, 0.999))\n","\n","    for epoch in range(num_epochs):\n","        for i, data in enumerate(dataloader):\n","            real_A = data\n","            real_A = real_A.to(device)\n","\n","            # Generator A->B 업데이트 (스케치 -> 얼굴 사진)\n","            optimizer_g.zero_grad()\n","            fake_B = cyclegan.generator_AtoB(real_A)\n","            loss_G_AtoB = adversarial_loss(cyclegan.discriminator_B(fake_B), torch.ones_like(fake_B)) \\\n","                          + cycle_consistency_loss(real_A, cyclegan.generator_BtoA(fake_B))\n","            loss_G_AtoB.backward()\n","            optimizer_g.step()\n","\n","            # Discriminator A 업데이트\n","            optimizer_d.zero_grad()\n","            loss_D_A = adversarial_loss(cyclegan.discriminator_A(real_A), torch.ones_like(real_A)) \\\n","                       + adversarial_loss(cyclegan.discriminator_A(fake_B.detach()), torch.zeros_like(fake_B))\n","            loss_D_A.backward()\n","            optimizer_d.step()\n","\n","            # Discriminator B 업데이트\n","            optimizer_d.zero_grad()\n","            loss_D_B = adversarial_loss(cyclegan.discriminator_B(fake_B), torch.ones_like(fake_B)) \\\n","                       + adversarial_loss(cyclegan.discriminator_B(fake_B.detach()), torch.zeros_like(fake_B))\n","            loss_D_B.backward()\n","            optimizer_d.step()\n","\n","            if i % 100 == 0:\n","                print(f\"[Epoch {epoch}/{num_epochs}] [Batch {i}/{len(dataloader)}] \"\n","                      f\"[D loss: {loss_D_A.item() + loss_D_B.item()}] \"\n","                      f\"[G loss: {loss_G_AtoB.item()}]\")\n","\n","        # 중간 결과 저장\n","        if epoch % 10 == 0:\n","            save_image(fake_B.data[:25], f\"/content/drive/MyDrive/GANFACE/generated_images_epoch_{epoch}.png\", nrow=5, normalize=True)\n","\n","    # 훈련 완료 후 모델 저장\n","    torch.save(cyclegan.state_dict(), '/content/drive/MyDrive/GANFACE/cyclegan_model.pth')\n","    print(\"Model saved successfully!\")\n","\n","# GPU 설정\n","device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n","\n","# CycleGAN 모델 인스턴스화\n","cyclegan = CycleGAN(input_channels=3, output_channels=3).to(device)\n","\n","# 훈련 시작\n","train(cyclegan, train_dataloader, num_epochs=200, device=device)\n"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":703},"executionInfo":{"elapsed":56018,"status":"error","timestamp":1733897730642,"user":{"displayName":"현우","userId":"14235240467528686656"},"user_tz":-540},"id":"5Dw3CzncWp8x","outputId":"d6cf0639-b911-4c9e-c75d-66f2b3d0a8c6"},"outputs":[{"name":"stdout","output_type":"stream","text":["Mounted at /content/drive\n","Requirement already satisfied: torch in /usr/local/lib/python3.10/dist-packages (2.5.1+cu121)\n","Requirement already satisfied: torchvision in /usr/local/lib/python3.10/dist-packages (0.20.1+cu121)\n","Requirement already satisfied: matplotlib in /usr/local/lib/python3.10/dist-packages (3.8.0)\n","Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (1.26.4)\n","Requirement already satisfied: pillow in /usr/local/lib/python3.10/dist-packages (11.0.0)\n","Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch) (3.16.1)\n","Requirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.10/dist-packages (from torch) (4.12.2)\n","Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch) (3.4.2)\n","Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch) (3.1.4)\n","Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch) (2024.10.0)\n","Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.10/dist-packages (from torch) (1.13.1)\n","Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy==1.13.1->torch) (1.3.0)\n","Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (1.3.1)\n","Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (0.12.1)\n","Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (4.55.1)\n","Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (1.4.7)\n","Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (24.2)\n","Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (3.2.0)\n","Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (2.8.2)\n","Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.7->matplotlib) (1.16.0)\n","Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch) (3.0.2)\n"]},{"ename":"RuntimeError","evalue":"The size of tensor a (15) must match the size of tensor b (256) at non-singleton dimension 3","output_type":"error","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)","\u001b[0;32m<ipython-input-1-d255ac27560a>\u001b[0m in \u001b[0;36m<cell line: 216>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    214\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    215\u001b[0m \u001b[0;31m# 훈련 시작\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 216\u001b[0;31m \u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcyclegan\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_dataloader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_epochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m200\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m","\u001b[0;32m<ipython-input-1-d255ac27560a>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(cyclegan, dataloader, num_epochs, device)\u001b[0m\n\u001b[1;32m    175\u001b[0m             \u001b[0moptimizer_g\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    176\u001b[0m             \u001b[0mfake_B\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcyclegan\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgenerator_AtoB\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mreal_A\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 177\u001b[0;31m             \u001b[0mloss_G_AtoB\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0madversarial_loss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcyclegan\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdiscriminator_B\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfake_B\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mones_like\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfake_B\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m\\\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    178\u001b[0m                           \u001b[0;34m+\u001b[0m \u001b[0mcycle_consistency_loss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mreal_A\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcyclegan\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgenerator_BtoA\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfake_B\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    179\u001b[0m             \u001b[0mloss_G_AtoB\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m<ipython-input-1-d255ac27560a>\u001b[0m in \u001b[0;36madversarial_loss\u001b[0;34m(disc_pred, target)\u001b[0m\n\u001b[1;32m    157\u001b[0m \u001b[0;31m# 수정된 Loss 함수\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    158\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0madversarial_loss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdisc_pred\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 159\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmean\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdisc_pred\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mtarget\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m**\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    160\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    161\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mcycle_consistency_loss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mreal_image\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreconstructed_image\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mRuntimeError\u001b[0m: The size of tensor a (15) must match the size of tensor b (256) at non-singleton dimension 3"]}],"source":["# Google Drive 마운트\n","from google.colab import drive\n","drive.mount('/content/drive')\n","\n","# 필요한 라이브러리 설치\n","!pip install torch torchvision matplotlib numpy pillow\n","\n","import os\n","import torch\n","import torch.nn as nn\n","import torch.nn.functional as F\n","from torchvision import transforms\n","from torch.utils.data import DataLoader, Dataset\n","from PIL import Image\n","import matplotlib.pyplot as plt\n","from torchvision.utils import save_image\n","\n","# Google Drive에서 이미지 경로 설정\n","image_folder_path = '/content/drive/MyDrive/GANFACE/filtered_images'\n","\n","# 이미지 전처리 설정 (Resize, Normalize, ToTensor 등)\n","transform = transforms.Compose([\n","    transforms.Resize(256),  # 256x256 크기로 리사이즈\n","    transforms.ToTensor(),   # 텐서로 변환\n","    transforms.Normalize((0.5,), (0.5,)),  # 정규화 (0.5, 0.5)로 예시\n","])\n","\n","# 사용자 정의 Dataset 클래스\n","class CustomImageDataset(Dataset):\n","    def __init__(self, image_folder_path, transform=None):\n","        self.image_folder_path = image_folder_path\n","        self.transform = transform\n","        self.image_paths = [os.path.join(image_folder_path, fname) for fname in os.listdir(image_folder_path) if fname.endswith('.png') or fname.endswith('.jpg')]\n","\n","    def __len__(self):\n","        return len(self.image_paths)\n","\n","    def __getitem__(self, idx):\n","        image_path = self.image_paths[idx]\n","        image = Image.open(image_path).convert('RGB')  # 이미지 열기 (RGB로 변환)\n","\n","        if self.transform:\n","            image = self.transform(image)  # 전처리\n","\n","        return image\n","\n","# CustomDataset을 사용하여 데이터셋 로드\n","train_dataset = CustomImageDataset(image_folder_path=image_folder_path, transform=transform)\n","\n","# DataLoader 설정 (배치 사이즈 16)\n","train_dataloader = DataLoader(train_dataset, batch_size=16, shuffle=True)\n","\n","# Self-Attention Layer\n","class SelfAttention(nn.Module):\n","    def __init__(self, in_channels):\n","        super(SelfAttention, self).__init__()\n","\n","        self.query_conv = nn.Conv2d(in_channels, in_channels // 8, kernel_size=1)\n","        self.key_conv = nn.Conv2d(in_channels, in_channels // 8, kernel_size=1)\n","        self.value_conv = nn.Conv2d(in_channels, in_channels, kernel_size=1)\n","        self.gamma = nn.Parameter(torch.zeros(1))\n","\n","    def forward(self, x):\n","        batch_size, channels, height, width = x.size()\n","\n","        query = self.query_conv(x).view(batch_size, -1, height * width)\n","        key = self.key_conv(x).view(batch_size, -1, height * width)\n","        value = self.value_conv(x).view(batch_size, -1, height * width)\n","\n","        attention = torch.bmm(query.permute(0, 2, 1), key)\n","        attention = F.softmax(attention, dim=-1)\n","\n","        out = torch.bmm(value, attention.permute(0, 2, 1))\n","        out = out.view(batch_size, channels, height, width)\n","\n","        return self.gamma * out + x\n","\n","# Spatial Attention Layer\n","class SpatialAttention(nn.Module):\n","    def __init__(self, in_channels):\n","        super(SpatialAttention, self).__init__()\n","        self.conv1 = nn.Conv2d(in_channels, 1, kernel_size=7, stride=1, padding=3)\n","\n","    def forward(self, x):\n","        attention_map = torch.sigmoid(self.conv1(x))\n","        return x * attention_map\n","\n","# Generator 모델 정의\n","class GeneratorWithAttention(nn.Module):\n","    def __init__(self, input_channels, output_channels):\n","        super(GeneratorWithAttention, self).__init__()\n","\n","        # Encoder\n","        self.encoder = nn.Sequential(\n","            nn.Conv2d(input_channels, 64, kernel_size=4, stride=2, padding=1),\n","            nn.ReLU(),\n","            nn.Conv2d(64, 128, kernel_size=4, stride=2, padding=1),\n","            nn.ReLU(),\n","            nn.Conv2d(128, 256, kernel_size=4, stride=2, padding=1),\n","            nn.ReLU(),\n","            SelfAttention(256),\n","            nn.Conv2d(256, 512, kernel_size=4, stride=2, padding=1),\n","            nn.ReLU(),\n","            SpatialAttention(512),\n","        )\n","\n","        # Decoder\n","        self.decoder = nn.Sequential(\n","            nn.ConvTranspose2d(512, 256, kernel_size=4, stride=2, padding=1),\n","            nn.ReLU(),\n","            nn.ConvTranspose2d(256, 128, kernel_size=4, stride=2, padding=1),\n","            nn.ReLU(),\n","            nn.ConvTranspose2d(128, 64, kernel_size=4, stride=2, padding=1),\n","            nn.ReLU(),\n","            nn.ConvTranspose2d(64, output_channels, kernel_size=4, stride=2, padding=1),\n","            nn.Tanh(),\n","        )\n","\n","    def forward(self, x):\n","        x = self.encoder(x)\n","        x = self.decoder(x)\n","        return x\n","\n","# Discriminator 모델 정의\n","class Discriminator(nn.Module):\n","    def __init__(self, input_channels):\n","        super(Discriminator, self).__init__()\n","\n","        self.model = nn.Sequential(\n","            nn.Conv2d(input_channels, 64, kernel_size=4, stride=2, padding=1),\n","            nn.LeakyReLU(0.2),\n","            nn.Conv2d(64, 128, kernel_size=4, stride=2, padding=1),\n","            nn.LeakyReLU(0.2),\n","            nn.Conv2d(128, 256, kernel_size=4, stride=2, padding=1),\n","            nn.LeakyReLU(0.2),\n","            nn.Conv2d(256, 512, kernel_size=4, stride=2, padding=1),\n","            nn.LeakyReLU(0.2),\n","            nn.Conv2d(512, 1, kernel_size=4, stride=1, padding=1),\n","            nn.Sigmoid(),\n","        )\n","\n","    def forward(self, x):\n","        return self.model(x)\n","\n","# CycleGAN 모델 정의\n","class CycleGAN(nn.Module):\n","    def __init__(self, input_channels, output_channels):\n","        super(CycleGAN, self).__init__()\n","        self.generator_AtoB = GeneratorWithAttention(input_channels, output_channels)\n","        self.generator_BtoA = GeneratorWithAttention(output_channels, input_channels)\n","        self.discriminator_A = Discriminator(input_channels)\n","        self.discriminator_B = Discriminator(output_channels)\n","\n","    def forward(self, x):\n","        pass  # forward()는 실제로 필요없음\n","\n","# 수정된 Loss 함수\n","def adversarial_loss(disc_pred, target):\n","    return torch.mean((disc_pred - target) ** 2)\n","\n","def cycle_consistency_loss(real_image, reconstructed_image):\n","    return torch.mean((real_image - reconstructed_image) ** 2)\n","\n","# 훈련 루프\n","def train(cyclegan, dataloader, num_epochs, device):\n","    optimizer_g = torch.optim.Adam(cyclegan.parameters(), lr=0.0002, betas=(0.5, 0.999))\n","    optimizer_d = torch.optim.Adam(cyclegan.parameters(), lr=0.0002, betas=(0.5, 0.999))\n","\n","    for epoch in range(num_epochs):\n","        for i, data in enumerate(dataloader):\n","            real_A = data\n","            real_A = real_A.to(device)\n","\n","            # Generator A->B 업데이트 (스케치 -> 얼굴 사진)\n","            optimizer_g.zero_grad()\n","            fake_B = cyclegan.generator_AtoB(real_A)\n","            loss_G_AtoB = adversarial_loss(cyclegan.discriminator_B(fake_B), torch.ones_like(fake_B)) \\\n","                          + cycle_consistency_loss(real_A, cyclegan.generator_BtoA(fake_B))\n","            loss_G_AtoB.backward()\n","            optimizer_g.step()\n","\n","            # Discriminator A 업데이트\n","            optimizer_d.zero_grad()\n","            loss_D_A = adversarial_loss(cyclegan.discriminator_A(real_A), torch.ones_like(real_A)) \\\n","                       + adversarial_loss(cyclegan.discriminator_A(fake_B.detach()), torch.zeros_like(fake_B))\n","            loss_D_A.backward()\n","            optimizer_d.step()\n","\n","            # Discriminator B 업데이트\n","            optimizer_d.zero_grad()\n","            loss_D_B = adversarial_loss(cyclegan.discriminator_B(fake_B), torch.ones_like(fake_B)) \\\n","                       + adversarial_loss(cyclegan.discriminator_B(fake_B.detach()), torch.zeros_like(fake_B))\n","            loss_D_B.backward()\n","            optimizer_d.step()\n","\n","            if i % 100 == 0:\n","                print(f\"[Epoch {epoch}/{num_epochs}] [Batch {i}/{len(dataloader)}] \"\n","                      f\"[D loss: {loss_D_A.item() + loss_D_B.item()}] \"\n","                      f\"[G loss: {loss_G_AtoB.item()}]\")\n","\n","        # 중간 결과 저장\n","        if epoch % 10 == 0:\n","            save_image(fake_B.data[:25], f\"/content/drive/MyDrive/GANFACE/generated_images_epoch_{epoch}.png\", nrow=5, normalize=True)\n","\n","    # 훈련 완료 후 모델 저장\n","    torch.save(cyclegan.state_dict(), '/content/drive/MyDrive/GANFACE/cyclegan_model.pth')\n","    print(\"Model saved successfully!\")\n","\n","# GPU 설정\n","device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n","\n","# CycleGAN 모델 인스턴스화\n","cyclegan = CycleGAN(input_channels=3, output_channels=3).to(device)\n","\n","# 훈련 시작\n","train(cyclegan, train_dataloader, num_epochs=200, device=device)\n"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":356},"executionInfo":{"elapsed":817,"status":"error","timestamp":1733897840556,"user":{"displayName":"현우","userId":"14235240467528686656"},"user_tz":-540},"id":"Yo9I9744XTxR","outputId":"b270b7cd-4c36-4479-bf1d-432b0ae2452f"},"outputs":[{"ename":"FileNotFoundError","evalue":"Couldn't find any class folder in /content/drive/MyDrive/GANFACE/filtered_images.","output_type":"error","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)","\u001b[0;32m<ipython-input-3-f30db804aadf>\u001b[0m in \u001b[0;36m<cell line: 150>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    148\u001b[0m ])\n\u001b[1;32m    149\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 150\u001b[0;31m \u001b[0mtrain_dataset\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdatasets\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mImageFolder\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mroot\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mimage_folder_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtransform\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtransform\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    151\u001b[0m \u001b[0mtrain_dataloader\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mDataLoader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_dataset\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m16\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mshuffle\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    152\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torchvision/datasets/folder.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, root, transform, target_transform, loader, is_valid_file, allow_empty)\u001b[0m\n\u001b[1;32m    326\u001b[0m         \u001b[0mallow_empty\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mbool\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    327\u001b[0m     ):\n\u001b[0;32m--> 328\u001b[0;31m         super().__init__(\n\u001b[0m\u001b[1;32m    329\u001b[0m             \u001b[0mroot\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    330\u001b[0m             \u001b[0mloader\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torchvision/datasets/folder.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, root, loader, extensions, transform, target_transform, is_valid_file, allow_empty)\u001b[0m\n\u001b[1;32m    147\u001b[0m     ) -> None:\n\u001b[1;32m    148\u001b[0m         \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mroot\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtransform\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtransform\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_transform\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtarget_transform\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 149\u001b[0;31m         \u001b[0mclasses\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mclass_to_idx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfind_classes\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mroot\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    150\u001b[0m         samples = self.make_dataset(\n\u001b[1;32m    151\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mroot\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torchvision/datasets/folder.py\u001b[0m in \u001b[0;36mfind_classes\u001b[0;34m(self, directory)\u001b[0m\n\u001b[1;32m    232\u001b[0m             \u001b[0;34m(\u001b[0m\u001b[0mTuple\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mList\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mDict\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mint\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mList\u001b[0m \u001b[0mof\u001b[0m \u001b[0mall\u001b[0m \u001b[0mclasses\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mdictionary\u001b[0m \u001b[0mmapping\u001b[0m \u001b[0meach\u001b[0m \u001b[0;32mclass\u001b[0m \u001b[0mto\u001b[0m \u001b[0man\u001b[0m \u001b[0mindex\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    233\u001b[0m         \"\"\"\n\u001b[0;32m--> 234\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mfind_classes\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdirectory\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    235\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    236\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__getitem__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mindex\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mint\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mTuple\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mAny\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mAny\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torchvision/datasets/folder.py\u001b[0m in \u001b[0;36mfind_classes\u001b[0;34m(directory)\u001b[0m\n\u001b[1;32m     41\u001b[0m     \u001b[0mclasses\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msorted\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mentry\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mname\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mentry\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mscandir\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdirectory\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mentry\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_dir\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     42\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mclasses\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 43\u001b[0;31m         \u001b[0;32mraise\u001b[0m \u001b[0mFileNotFoundError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"Couldn't find any class folder in {directory}.\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     44\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     45\u001b[0m     \u001b[0mclass_to_idx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0mcls_name\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcls_name\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mclasses\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mFileNotFoundError\u001b[0m: Couldn't find any class folder in /content/drive/MyDrive/GANFACE/filtered_images."]}],"source":["import torch\n","import torch.nn as nn\n","import torch.optim as optim\n","from torch.utils.data import DataLoader\n","from torchvision import datasets, transforms\n","from torchvision.utils import save_image\n","import os\n","\n","# 필요한 모델 및 함수들 정의\n","class Generator(nn.Module):\n","    def __init__(self):\n","        super(Generator, self).__init__()\n","        self.model = nn.Sequential(\n","            nn.Conv2d(3, 64, 4, 2, 1),  # 첫 번째 Conv Layer\n","            nn.ReLU(True),\n","            nn.Conv2d(64, 128, 4, 2, 1),\n","            nn.BatchNorm2d(128),\n","            nn.ReLU(True),\n","            nn.Conv2d(128, 256, 4, 2, 1),\n","            nn.BatchNorm2d(256),\n","            nn.ReLU(True),\n","            nn.Conv2d(256, 512, 4, 2, 1),\n","            nn.BatchNorm2d(512),\n","            nn.ReLU(True),\n","            nn.Conv2d(512, 3, 4, 2, 1),\n","            nn.Tanh()\n","        )\n","\n","    def forward(self, x):\n","        return self.model(x)\n","\n","class Discriminator(nn.Module):\n","    def __init__(self):\n","        super(Discriminator, self).__init__()\n","        self.model = nn.Sequential(\n","            nn.Conv2d(3, 64, 4, 2, 1),\n","            nn.LeakyReLU(0.2, inplace=True),\n","            nn.Conv2d(64, 128, 4, 2, 1),\n","            nn.BatchNorm2d(128),\n","            nn.LeakyReLU(0.2, inplace=True),\n","            nn.Conv2d(128, 256, 4, 2, 1),\n","            nn.BatchNorm2d(256),\n","            nn.LeakyReLU(0.2, inplace=True),\n","            nn.Conv2d(256, 1, 4, 1, 1),\n","            nn.Sigmoid()\n","        )\n","\n","    def forward(self, x):\n","        return self.model(x)\n","\n","# CycleGAN 모델\n","class CycleGAN(nn.Module):\n","    def __init__(self):\n","        super(CycleGAN, self).__init__()\n","        self.generator_AtoB = Generator()\n","        self.generator_BtoA = Generator()\n","        self.discriminator_A = Discriminator()\n","        self.discriminator_B = Discriminator()\n","\n","    def forward(self, x, reverse=False):\n","        if reverse:\n","            return self.generator_BtoA(x)\n","        return self.generator_AtoB(x)\n","\n","# Loss 함수\n","def adversarial_loss(disc_pred, target):\n","    target = target.view_as(disc_pred)  # target 크기 맞추기\n","    return torch.mean((disc_pred - target) ** 2)\n","\n","def cycle_consistency_loss(real_image, reconstructed_image):\n","    return torch.mean((real_image - reconstructed_image) ** 2)\n","\n","# 훈련 루프\n","def train(cyclegan, dataloader, num_epochs, device):\n","    optimizer_g = torch.optim.Adam(list(cyclegan.generator_AtoB.parameters()) + list(cyclegan.generator_BtoA.parameters()), lr=0.0002, betas=(0.5, 0.999))\n","    optimizer_d_A = torch.optim.Adam(cyclegan.discriminator_A.parameters(), lr=0.0002, betas=(0.5, 0.999))\n","    optimizer_d_B = torch.optim.Adam(cyclegan.discriminator_B.parameters(), lr=0.0002, betas=(0.5, 0.999))\n","\n","    for epoch in range(num_epochs):\n","        for i, data in enumerate(dataloader):\n","            real_A = data\n","            real_A = real_A.to(device)\n","\n","            # Generator A->B 업데이트 (스케치 -> 얼굴 사진)\n","            optimizer_g.zero_grad()\n","            fake_B = cyclegan(real_A)\n","            disc_pred_fake = cyclegan.discriminator_B(fake_B)\n","            loss_G_AtoB = adversarial_loss(disc_pred_fake, torch.ones_like(disc_pred_fake))\n","\n","            # Generator B->A 업데이트 (얼굴 사진 -> 스케치)\n","            fake_A = cyclegan(real_A, reverse=True)\n","            disc_pred_fake_A = cyclegan.discriminator_A(fake_A)\n","            loss_G_BtoA = adversarial_loss(disc_pred_fake_A, torch.ones_like(disc_pred_fake_A))\n","\n","            # Cycle consistency loss\n","            reconstructed_A = cyclegan(fake_B, reverse=True)\n","            reconstructed_B = cyclegan(fake_A)\n","            loss_cycle_A = cycle_consistency_loss(real_A, reconstructed_A)\n","            loss_cycle_B = cycle_consistency_loss(real_A, reconstructed_B)\n","\n","            loss_G = loss_G_AtoB + loss_G_BtoA + 10 * (loss_cycle_A + loss_cycle_B)\n","            loss_G.backward()\n","            optimizer_g.step()\n","\n","            # Discriminator A 업데이트\n","            optimizer_d_A.zero_grad()\n","            disc_pred_real_A = cyclegan.discriminator_A(real_A)\n","            loss_D_A_real = adversarial_loss(disc_pred_real_A, torch.ones_like(disc_pred_real_A))\n","\n","            disc_pred_fake_A = cyclegan.discriminator_A(fake_A.detach())\n","            loss_D_A_fake = adversarial_loss(disc_pred_fake_A, torch.zeros_like(disc_pred_fake_A))\n","\n","            loss_D_A = (loss_D_A_real + loss_D_A_fake) / 2\n","            loss_D_A.backward()\n","            optimizer_d_A.step()\n","\n","            # Discriminator B 업데이트\n","            optimizer_d_B.zero_grad()\n","            disc_pred_real_B = cyclegan.discriminator_B(real_B)\n","            loss_D_B_real = adversarial_loss(disc_pred_real_B, torch.ones_like(disc_pred_real_B))\n","\n","            disc_pred_fake_B = cyclegan.discriminator_B(fake_B.detach())\n","            loss_D_B_fake = adversarial_loss(disc_pred_fake_B, torch.zeros_like(disc_pred_fake_B))\n","\n","            loss_D_B = (loss_D_B_real + loss_D_B_fake) / 2\n","            loss_D_B.backward()\n","            optimizer_d_B.step()\n","\n","            if i % 100 == 0:\n","                print(f\"Epoch [{epoch+1}/{num_epochs}], Step [{i+1}/{len(dataloader)}], \"\n","                      f\"Loss D_A: {loss_D_A.item():.4f}, Loss D_B: {loss_D_B.item():.4f}, \"\n","                      f\"Loss G: {loss_G.item():.4f}\")\n","\n","        # 이미지 저장\n","        if (epoch + 1) % 10 == 0:\n","            save_image(fake_B, f'/content/drive/MyDrive/GANFACE/fake_B_epoch_{epoch+1}.png')\n","            save_image(fake_A, f'/content/drive/MyDrive/GANFACE/fake_A_epoch_{epoch+1}.png')\n","\n","# 데이터셋 경로\n","image_folder_path = '/content/drive/MyDrive/GANFACE/filtered_images'\n","\n","# 이미지 변환 및 데이터셋 로딩\n","transform = transforms.Compose([\n","    transforms.Resize(256),\n","    transforms.CenterCrop(256),\n","    transforms.ToTensor(),\n","    transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))\n","])\n","\n","train_dataset = datasets.ImageFolder(root=image_folder_path, transform=transform)\n","train_dataloader = DataLoader(train_dataset, batch_size=16, shuffle=True)\n","\n","# 장치 설정 (GPU 사용 여부)\n","device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n","\n","# CycleGAN 모델 초기화\n","cyclegan = CycleGAN().to(device)\n","\n","# 모델 훈련\n","train(cyclegan, train_dataloader, num_epochs=200, device=device)\n","\n","# 모델 저장\n","torch.save(cyclegan.state_dict(), '/content/drive/MyDrive/GANFACE/cyclegan.pth')\n"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":356},"executionInfo":{"elapsed":2308,"status":"error","timestamp":1733898067398,"user":{"displayName":"현우","userId":"14235240467528686656"},"user_tz":-540},"id":"Ucmfsfv9Xldv","outputId":"1b192fa6-d066-4757-bc71-c5a48d69a6de"},"outputs":[{"ename":"RuntimeError","evalue":"Calculated padded input size per channel: (3 x 3). Kernel size: (4 x 4). Kernel size can't be greater than actual input size","output_type":"error","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)","\u001b[0;32m<ipython-input-7-501860549779>\u001b[0m in \u001b[0;36m<cell line: 181>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    179\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    180\u001b[0m \u001b[0;31m# 모델 훈련\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 181\u001b[0;31m \u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcyclegan\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_dataloader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_epochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m200\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    182\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    183\u001b[0m \u001b[0;31m# 모델 저장\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m<ipython-input-7-501860549779>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(cyclegan, dataloader, num_epochs, device)\u001b[0m\n\u001b[1;32m    126\u001b[0m             \u001b[0moptimizer_g\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    127\u001b[0m             \u001b[0mfake_B\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcyclegan\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mreal_A\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 128\u001b[0;31m             \u001b[0mdisc_pred_fake\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcyclegan\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdiscriminator_B\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfake_B\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    129\u001b[0m             \u001b[0mloss_G_AtoB\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0madversarial_loss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdisc_pred_fake\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mones_like\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdisc_pred_fake\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    130\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1734\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1735\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1736\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1737\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1738\u001b[0m     \u001b[0;31m# torchrec tests the code consistency with the following code\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1745\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1746\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1747\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1748\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1749\u001b[0m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m<ipython-input-7-501860549779>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     49\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     50\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 51\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     52\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     53\u001b[0m \u001b[0;31m# CycleGAN 모델 정의\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1734\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1735\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1736\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1737\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1738\u001b[0m     \u001b[0;31m# torchrec tests the code consistency with the following code\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1745\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1746\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1747\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1748\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1749\u001b[0m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/container.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    248\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    249\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mmodule\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 250\u001b[0;31m             \u001b[0minput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodule\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    251\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    252\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1734\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1735\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1736\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1737\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1738\u001b[0m     \u001b[0;31m# torchrec tests the code consistency with the following code\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1745\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1746\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1747\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1748\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1749\u001b[0m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/conv.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    552\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    553\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 554\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_conv_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbias\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    555\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    556\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/conv.py\u001b[0m in \u001b[0;36m_conv_forward\u001b[0;34m(self, input, weight, bias)\u001b[0m\n\u001b[1;32m    547\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgroups\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    548\u001b[0m             )\n\u001b[0;32m--> 549\u001b[0;31m         return F.conv2d(\n\u001b[0m\u001b[1;32m    550\u001b[0m             \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbias\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstride\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpadding\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdilation\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgroups\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    551\u001b[0m         )\n","\u001b[0;31mRuntimeError\u001b[0m: Calculated padded input size per channel: (3 x 3). Kernel size: (4 x 4). Kernel size can't be greater than actual input size"]}],"source":["import torch\n","import torch.nn as nn\n","import torch.optim as optim\n","from torch.utils.data import Dataset, DataLoader\n","from torchvision import transforms\n","from torchvision.utils import save_image\n","from PIL import Image\n","import os\n","\n","# Generator 모델 정의\n","class Generator(nn.Module):\n","    def __init__(self):\n","        super(Generator, self).__init__()\n","        self.model = nn.Sequential(\n","            nn.Conv2d(3, 64, 4, 2, 1),  # 첫 번째 Conv Layer\n","            nn.ReLU(True),\n","            nn.Conv2d(64, 128, 4, 2, 1),\n","            nn.BatchNorm2d(128),\n","            nn.ReLU(True),\n","            nn.Conv2d(128, 256, 4, 2, 1),\n","            nn.BatchNorm2d(256),\n","            nn.ReLU(True),\n","            nn.Conv2d(256, 512, 4, 2, 1),\n","            nn.BatchNorm2d(512),\n","            nn.ReLU(True),\n","            nn.Conv2d(512, 3, 4, 2, 1),\n","            nn.Tanh()\n","        )\n","\n","    def forward(self, x):\n","        return self.model(x)\n","\n","# Discriminator 모델 정의\n","class Discriminator(nn.Module):\n","    def __init__(self):\n","        super(Discriminator, self).__init__()\n","        self.model = nn.Sequential(\n","            nn.Conv2d(3, 64, 4, 2, 1),\n","            nn.LeakyReLU(0.2, inplace=True),\n","            nn.Conv2d(64, 128, 4, 2, 1),\n","            nn.BatchNorm2d(128),\n","            nn.LeakyReLU(0.2, inplace=True),\n","            nn.Conv2d(128, 256, 4, 2, 1),\n","            nn.BatchNorm2d(256),\n","            nn.LeakyReLU(0.2, inplace=True),\n","            nn.Conv2d(256, 1, 4, 1, 1),\n","            nn.Sigmoid()\n","        )\n","\n","    def forward(self, x):\n","        return self.model(x)\n","\n","# CycleGAN 모델 정의\n","class CycleGAN(nn.Module):\n","    def __init__(self):\n","        super(CycleGAN, self).__init__()\n","        self.generator_AtoB = Generator()\n","        self.generator_BtoA = Generator()\n","        self.discriminator_A = Discriminator()\n","        self.discriminator_B = Discriminator()\n","\n","    def forward(self, x, reverse=False):\n","        if reverse:\n","            return self.generator_BtoA(x)\n","        return self.generator_AtoB(x)\n","\n","# Loss 함수 정의\n","def adversarial_loss(disc_pred, target):\n","    target = target.view_as(disc_pred)  # target 크기 맞추기\n","    return torch.mean((disc_pred - target) ** 2)\n","\n","def cycle_consistency_loss(real_image, reconstructed_image):\n","    return torch.mean((real_image - reconstructed_image) ** 2)\n","\n","# 커스텀 데이터셋 클래스 정의\n","class CustomImageDataset(Dataset):\n","    def __init__(self, image_folder_path, transform=None):\n","        self.image_folder_path = image_folder_path\n","        self.transform = transform\n","        self.image_paths = [os.path.join(image_folder_path, fname) for fname in os.listdir(image_folder_path) if fname.endswith('.png') or fname.endswith('.jpg')]\n","\n","    def __len__(self):\n","        return len(self.image_paths)\n","\n","    def __getitem__(self, idx):\n","        image_path = self.image_paths[idx]\n","        image = Image.open(image_path).convert('RGB')\n","\n","        if self.transform:\n","            image = self.transform(image)\n","\n","        return image\n","\n","# 데이터셋 경로\n","image_folder_path = '/content/drive/MyDrive/GANFACE/filtered_images'\n","\n","# 이미지 변환 및 데이터셋 로딩 (크기를 128x128으로 설정)\n","transform = transforms.Compose([\n","    transforms.Resize(128),  # 이미지를 128x128으로 크기 조정\n","    transforms.CenterCrop(128),\n","    transforms.ToTensor(),\n","    transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))\n","])\n","\n","train_dataset = CustomImageDataset(image_folder_path, transform=transform)\n","train_dataloader = DataLoader(train_dataset, batch_size=16, shuffle=True)\n","\n","# 장치 설정 (GPU 사용 여부)\n","device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n","\n","# CycleGAN 모델 초기화\n","cyclegan = CycleGAN().to(device)\n","\n","# 훈련 루프\n","def train(cyclegan, dataloader, num_epochs, device):\n","    optimizer_g = torch.optim.Adam(list(cyclegan.generator_AtoB.parameters()) + list(cyclegan.generator_BtoA.parameters()), lr=0.0002, betas=(0.5, 0.999))\n","    optimizer_d_A = torch.optim.Adam(cyclegan.discriminator_A.parameters(), lr=0.0002, betas=(0.5, 0.999))\n","    optimizer_d_B = torch.optim.Adam(cyclegan.discriminator_B.parameters(), lr=0.0002, betas=(0.5, 0.999))\n","\n","    for epoch in range(num_epochs):\n","        for i, data in enumerate(dataloader):\n","            real_A = data\n","            real_A = real_A.to(device)\n","\n","            # Generator A->B 업데이트 (스케치 -> 얼굴 사진)\n","            optimizer_g.zero_grad()\n","            fake_B = cyclegan(real_A)\n","            disc_pred_fake = cyclegan.discriminator_B(fake_B)\n","            loss_G_AtoB = adversarial_loss(disc_pred_fake, torch.ones_like(disc_pred_fake))\n","\n","            # Generator B->A 업데이트 (얼굴 사진 -> 스케치)\n","            fake_A = cyclegan(real_A, reverse=True)\n","            disc_pred_fake_A = cyclegan.discriminator_A(fake_A)\n","            loss_G_BtoA = adversarial_loss(disc_pred_fake_A, torch.ones_like(disc_pred_fake_A))\n","\n","            # Cycle consistency loss\n","            reconstructed_A = cyclegan(fake_B, reverse=True)\n","            reconstructed_B = cyclegan(fake_A)\n","            loss_cycle_A = cycle_consistency_loss(real_A, reconstructed_A)\n","            loss_cycle_B = cycle_consistency_loss(real_A, reconstructed_B)\n","\n","            loss_G = loss_G_AtoB + loss_G_BtoA + 10 * (loss_cycle_A + loss_cycle_B)\n","            loss_G.backward()\n","            optimizer_g.step()\n","\n","            # Discriminator A 업데이트\n","            optimizer_d_A.zero_grad()\n","            disc_pred_real_A = cyclegan.discriminator_A(real_A)\n","            loss_D_A_real = adversarial_loss(disc_pred_real_A, torch.ones_like(disc_pred_real_A))\n","\n","            disc_pred_fake_A = cyclegan.discriminator_A(fake_A.detach())\n","            loss_D_A_fake = adversarial_loss(disc_pred_fake_A, torch.zeros_like(disc_pred_fake_A))\n","\n","            loss_D_A = (loss_D_A_real + loss_D_A_fake) / 2\n","            loss_D_A.backward()\n","            optimizer_d_A.step()\n","\n","            # Discriminator B 업데이트\n","            optimizer_d_B.zero_grad()\n","            disc_pred_real_B = cyclegan.discriminator_B(real_B)\n","            loss_D_B_real = adversarial_loss(disc_pred_real_B, torch.ones_like(disc_pred_real_B))\n","\n","            disc_pred_fake_B = cyclegan.discriminator_B(fake_B.detach())\n","            loss_D_B_fake = adversarial_loss(disc_pred_fake_B, torch.zeros_like(disc_pred_fake_B))\n","\n","            loss_D_B = (loss_D_B_real + loss_D_B_fake) / 2\n","            loss_D_B.backward()\n","            optimizer_d_B.step()\n","\n","            if i % 100 == 0:\n","                print(f\"Epoch [{epoch+1}/{num_epochs}], Step [{i+1}/{len(dataloader)}], \"\n","                      f\"Loss D_A: {loss_D_A.item():.4f}, Loss D_B: {loss_D_B.item():.4f}, \"\n","                      f\"Loss G: {loss_G.item():.4f}\")\n","\n","        # 이미지 저장\n","        if (epoch + 1) % 10 == 0:\n","            save_image(fake_B, f'/content/drive/MyDrive/GANFACE/fake_B_epoch_{epoch+1}.png')\n","            save_image(fake_A, f'/content/drive/MyDrive/GANFACE/fake_A_epoch_{epoch+1}.png')\n","\n","# 모델 훈련\n","train(cyclegan, train_dataloader, num_epochs=200, device=device)\n","\n","# 모델 저장\n","torch.save(cyclegan.state_dict(), '/content/drive/MyDrive/GANFACE/cyclegan.pth')\n"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"background_save":true,"base_uri":"https://localhost:8080/","height":1000,"output_embedded_package_id":"10oNpZvUogK1we-1xmPgo9Kiir0ZXb3_Z"},"executionInfo":{"elapsed":498075,"status":"ok","timestamp":1733898875287,"user":{"displayName":"현우","userId":"14235240467528686656"},"user_tz":-540},"id":"Mby-HvAqYbtw","outputId":"468a76d6-634f-428e-9863-f26ddbbb7fbc"},"outputs":[{"output_type":"display_data","data":{"text/plain":"Output hidden; open in https://colab.research.google.com to view."},"metadata":{}}],"source":["import torch\n","import os\n","from torch import nn, optim\n","from torch.utils.data import Dataset, DataLoader\n","from torchvision import transforms\n","from PIL import Image\n","import matplotlib.pyplot as plt\n","from tqdm import tqdm\n","import torch.nn.functional as F\n","import torchvision  # 추가해야 하는 부분\n","\n","\n","# GPU 설정\n","device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n","\n","# 데이터셋 경로 설정\n","image_folder_path = '/content/drive/MyDrive/GANFACE/filtered_images'  # 이미지가 있는 폴더\n","\n","# 이미지 데이터 로딩을 위한 transform 설정\n","transform = transforms.Compose([\n","    transforms.Resize((256, 256)),  # 이미지를 256x256로 크기 조정\n","    transforms.ToTensor(),  # 이미지를 Tensor로 변환\n","    transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))  # 이미지 정규화\n","])\n","\n","# 커스텀 데이터셋 클래스 (ImageFolder 대신)\n","class CustomDataset(Dataset):\n","    def __init__(self, image_folder, transform=None):\n","        self.image_folder = image_folder\n","        self.transform = transform\n","        self.image_files = [f for f in os.listdir(image_folder) if f.endswith(('.jpg', '.png', '.jpeg'))]  # 이미지 파일 확장자에 맞게 필터링\n","\n","    def __len__(self):\n","        return len(self.image_files)\n","\n","    def __getitem__(self, idx):\n","        img_name = os.path.join(self.image_folder, self.image_files[idx])\n","        image = Image.open(img_name).convert(\"RGB\")  # 이미지를 RGB 모드로 열기\n","        if self.transform:\n","            image = self.transform(image)  # 변환 적용\n","        return image\n","\n","# 데이터셋 및 DataLoader 설정\n","train_dataset = CustomDataset(image_folder_path, transform=transform)\n","train_dataloader = DataLoader(train_dataset, batch_size=16, shuffle=True)\n","\n","# 간단한 모델 (예: 간단한 GAN 모델을 위한 Discriminator와 Generator)\n","class SimpleGenerator(nn.Module):\n","    def __init__(self):\n","        super(SimpleGenerator, self).__init__()\n","        self.fc = nn.Linear(100, 256*256*3)\n","\n","    def forward(self, z):\n","        out = self.fc(z)\n","        out = out.view(-1, 3, 256, 256)  # 256x256 RGB 이미지로 변환\n","        return torch.tanh(out)\n","\n","class SimpleDiscriminator(nn.Module):\n","    def __init__(self):\n","        super(SimpleDiscriminator, self).__init__()\n","        self.conv1 = nn.Conv2d(3, 64, kernel_size=4, stride=2, padding=1)\n","        self.conv2 = nn.Conv2d(64, 128, kernel_size=4, stride=2, padding=1)\n","        self.fc = nn.Linear(128 * 64 * 64, 1)\n","\n","    def forward(self, x):\n","        x = F.leaky_relu(self.conv1(x), 0.2)\n","        x = F.leaky_relu(self.conv2(x), 0.2)\n","        x = x.view(x.size(0), -1)\n","        x = self.fc(x)\n","        return torch.sigmoid(x)\n","\n","# 모델 초기화\n","generator = SimpleGenerator().to(device)\n","discriminator = SimpleDiscriminator().to(device)\n","\n","# 손실 함수 및 옵티마이저\n","adversarial_loss = nn.BCELoss()\n","optimizer_g = optim.Adam(generator.parameters(), lr=0.0002, betas=(0.5, 0.999))\n","optimizer_d = optim.Adam(discriminator.parameters(), lr=0.0002, betas=(0.5, 0.999))\n","\n","# 훈련 함수\n","def train(generator, discriminator, dataloader, num_epochs=100, device=device):\n","    for epoch in range(num_epochs):\n","        for real_images in tqdm(dataloader):\n","            real_images = real_images.to(device)\n","\n","            # 진짜 이미지에 대한 레이블 생성\n","            real_labels = torch.ones(real_images.size(0), 1).to(device)\n","            fake_labels = torch.zeros(real_images.size(0), 1).to(device)\n","\n","            # -----------------------\n","            #  Discriminator 학습\n","            # -----------------------\n","            optimizer_d.zero_grad()\n","\n","            # 진짜 이미지에 대한 loss\n","            output_real = discriminator(real_images)\n","            loss_d_real = adversarial_loss(output_real, real_labels)\n","\n","            # 가짜 이미지에 대한 loss\n","            z = torch.randn(real_images.size(0), 100).to(device)\n","            fake_images = generator(z)\n","            output_fake = discriminator(fake_images.detach())\n","            loss_d_fake = adversarial_loss(output_fake, fake_labels)\n","\n","            # 총 Discriminator loss\n","            loss_d = (loss_d_real + loss_d_fake) / 2\n","            loss_d.backward()\n","            optimizer_d.step()\n","\n","            # -----------------------\n","            #  Generator 학습\n","            # -----------------------\n","            optimizer_g.zero_grad()\n","\n","            # Generator의 목표는 Discriminator를 속이는 것\n","            output_fake = discriminator(fake_images)\n","            loss_g = adversarial_loss(output_fake, real_labels)  # 진짜 레이블을 fake에 대해 계산\n","            loss_g.backward()\n","            optimizer_g.step()\n","\n","        # Epoch마다 결과 출력\n","        print(f\"Epoch [{epoch+1}/{num_epochs}], Loss D: {loss_d.item()}, Loss G: {loss_g.item()}\")\n","\n","        if (epoch+1) % 10 == 0:  # 10 epoch마다 결과 이미지 저장\n","            save_fake_images(epoch+1, fake_images)\n","\n","# 이미지 저장 함수\n","def save_fake_images(epoch, fake_images):\n","    fake_images = fake_images.detach().cpu()\n","    grid = torchvision.utils.make_grid(fake_images, normalize=True)\n","    plt.figure(figsize=(8,8))\n","    plt.imshow(grid.permute(1, 2, 0))\n","    plt.axis('off')\n","    plt.savefig(f\"/content/drive/MyDrive/GANFACE/fake_images_epoch_{epoch}.png\")\n","\n","# 훈련 시작\n","train(generator, discriminator, train_dataloader, num_epochs=100, device=device)\n"]},{"cell_type":"code","source":["import os\n","import torch\n","import torchvision\n","import torch.nn as nn\n","import torch.optim as optim\n","import matplotlib.pyplot as plt\n","from torch.utils.data import DataLoader, Dataset\n","from torchvision import transforms\n","from PIL import Image\n","from tqdm import tqdm\n","\n","# 커스텀 데이터셋 클래스\n","class CustomImageDataset(Dataset):\n","    def __init__(self, image_folder_path, transform=None):\n","        self.image_paths = [os.path.join(image_folder_path, fname) for fname in os.listdir(image_folder_path)]\n","        self.transform = transform\n","\n","    def __len__(self):\n","        return len(self.image_paths)\n","\n","    def __getitem__(self, idx):\n","        img_path = self.image_paths[idx]\n","        image = Image.open(img_path).convert('RGB')  # 이미지를 RGB로 변환\n","\n","        if self.transform:\n","            image = self.transform(image)\n","\n","        return image\n","\n","# 모델 아키텍처 정의 (Generator, Discriminator)\n","class Generator(nn.Module):\n","    def __init__(self):\n","        super(Generator, self).__init__()\n","        self.conv1 = nn.Conv2d(1, 64, kernel_size=4, stride=2, padding=1)\n","        self.conv2 = nn.Conv2d(64, 128, kernel_size=4, stride=2, padding=1)\n","        self.fc1 = nn.Linear(128 * 7 * 7, 1024)\n","        self.fc2 = nn.Linear(1024, 128 * 7 * 7)\n","        self.deconv1 = nn.ConvTranspose2d(128, 64, kernel_size=4, stride=2, padding=1)\n","        self.deconv2 = nn.ConvTranspose2d(64, 1, kernel_size=4, stride=2, padding=1)\n","\n","    def forward(self, x):\n","        x = torch.relu(self.conv1(x))\n","        x = torch.relu(self.conv2(x))\n","        x = x.view(x.size(0), -1)\n","        x = torch.relu(self.fc1(x))\n","        x = torch.relu(self.fc2(x))\n","        x = x.view(x.size(0), 128, 7, 7)\n","        x = torch.relu(self.deconv1(x))\n","        x = torch.tanh(self.deconv2(x))\n","        return x\n","\n","class Discriminator(nn.Module):\n","    def __init__(self):\n","        super(Discriminator, self).__init__()\n","        self.conv1 = nn.Conv2d(1, 64, kernel_size=4, stride=2, padding=1)\n","        self.conv2 = nn.Conv2d(64, 128, kernel_size=4, stride=2, padding=1)\n","        self.fc1 = nn.Linear(128 * 7 * 7, 1024)\n","        self.fc2 = nn.Linear(1024, 1)\n","\n","    def forward(self, x):\n","        x = torch.relu(self.conv1(x))\n","        x = torch.relu(self.conv2(x))\n","        x = x.view(x.size(0), -1)\n","        x = torch.relu(self.fc1(x))\n","        x = torch.sigmoid(self.fc2(x))\n","        return x\n","\n","# Loss 함수 및 최적화기\n","def adversarial_loss(disc_pred, target):\n","    return torch.mean((disc_pred - target) ** 2)\n","\n","def save_fake_images(epoch, fake_images):\n","    fake_images = fake_images.detach().cpu()\n","    grid = torchvision.utils.make_grid(fake_images, normalize=True)\n","    plt.figure(figsize=(8,8))\n","    plt.imshow(grid.permute(1, 2, 0))\n","    plt.title(f\"Epoch {epoch}\")\n","    plt.axis('off')\n","    plt.savefig(f'fake_images_epoch_{epoch}.png')\n","    plt.close()\n","\n","# 데이터셋 준비\n","transform = transforms.Compose([\n","    transforms.Grayscale(num_output_channels=1),\n","    transforms.Resize((28, 28)),\n","    transforms.ToTensor(),\n","    transforms.Normalize((0.5,), (0.5,))\n","])\n","\n","image_folder_path = '/content/drive/MyDrive/GANFACE/filtered_images'  # 이미지 폴더 경로\n","train_dataset = CustomImageDataset(image_folder_path, transform=transform)\n","train_dataloader = DataLoader(train_dataset, batch_size=16, shuffle=True)\n","\n","# 모델 초기화\n","device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n","generator = Generator().to(device)\n","discriminator = Discriminator().to(device)\n","\n","# Optimizer 설정\n","optimizer_g = optim.Adam(generator.parameters(), lr=0.0002, betas=(0.5, 0.999))\n","optimizer_d = optim.Adam(discriminator.parameters(), lr=0.0002, betas=(0.5, 0.999))\n","\n","# 훈련 함수\n","def train(generator, discriminator, dataloader, num_epochs, device):\n","    for epoch in range(num_epochs):\n","        for i, real_images in enumerate(tqdm(dataloader)):\n","            real_images = real_images.to(device)\n","            batch_size = real_images.size(0)\n","\n","            # 실제 이미지에 대한 discriminator 예측\n","            optimizer_d.zero_grad()\n","            disc_pred_real = discriminator(real_images)\n","            loss_d_real = adversarial_loss(disc_pred_real, torch.ones_like(disc_pred_real))\n","\n","            # 가짜 이미지 생성\n","            noise = torch.randn(batch_size, 1, 28, 28).to(device)\n","            fake_images = generator(noise)\n","\n","            # 가짜 이미지에 대한 discriminator 예측\n","            disc_pred_fake = discriminator(fake_images.detach())\n","            loss_d_fake = adversarial_loss(disc_pred_fake, torch.zeros_like(disc_pred_fake))\n","\n","            # Discriminator loss\n","            loss_d = (loss_d_real + loss_d_fake) / 2\n","            loss_d.backward()\n","            optimizer_d.step()\n","\n","            # Generator 업데이트\n","            optimizer_g.zero_grad()\n","            disc_pred_fake = discriminator(fake_images)\n","            loss_g = adversarial_loss(disc_pred_fake, torch.ones_like(disc_pred_fake))\n","            loss_g.backward()\n","            optimizer_g.step()\n","\n","            # 주기적으로 훈련 상태 출력\n","            if i % 100 == 0:\n","                print(f\"Epoch [{epoch+1}/{num_epochs}], Step [{i+1}/{len(dataloader)}], Loss D: {loss_d.item()}, Loss G: {loss_g.item()}\")\n","\n","        # 각 epoch마다 fake 이미지 저장\n","        if epoch % 10 == 0:\n","            save_fake_images(epoch, fake_images)\n","\n","# 훈련 시작\n","num_epochs = 200  # 에폭 수 늘리기\n","train(generator, discriminator, train_dataloader, num_epochs, device)\n","\n","# 모델 저장\n","torch.save(generator.state_dict(), 'generator.pth')\n","torch.save(discriminator.state_dict(), 'discriminator.pth')\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"5eSdqi_-czJ7","executionInfo":{"status":"ok","timestamp":1733899900764,"user_tz":-540,"elapsed":575304,"user":{"displayName":"현우","userId":"14235240467528686656"}},"outputId":"7be85d15-77f2-48b1-89f1-8cbbc264f84f"},"execution_count":13,"outputs":[{"output_type":"stream","name":"stderr","text":["  8%|▊         | 3/40 [00:02<00:22,  1.62it/s]"]},{"output_type":"stream","name":"stdout","text":["Epoch [1/200], Step [1/40], Loss D: 0.24788686633110046, Loss G: 0.24651065468788147\n"]},{"output_type":"stream","name":"stderr","text":["100%|██████████| 40/40 [00:05<00:00,  7.96it/s]\n","  5%|▌         | 2/40 [00:00<00:03, 12.37it/s]"]},{"output_type":"stream","name":"stdout","text":["Epoch [2/200], Step [1/40], Loss D: 0.10102327913045883, Loss G: 0.9144269227981567\n"]},{"output_type":"stream","name":"stderr","text":["100%|██████████| 40/40 [00:02<00:00, 14.03it/s]\n","  5%|▌         | 2/40 [00:00<00:02, 14.17it/s]"]},{"output_type":"stream","name":"stdout","text":["Epoch [3/200], Step [1/40], Loss D: 0.20567619800567627, Loss G: 0.42981642484664917\n"]},{"output_type":"stream","name":"stderr","text":["100%|██████████| 40/40 [00:03<00:00, 12.99it/s]\n","  5%|▌         | 2/40 [00:00<00:02, 13.41it/s]"]},{"output_type":"stream","name":"stdout","text":["Epoch [4/200], Step [1/40], Loss D: 0.10919462144374847, Loss G: 0.712607204914093\n"]},{"output_type":"stream","name":"stderr","text":["100%|██████████| 40/40 [00:02<00:00, 13.58it/s]\n","  5%|▌         | 2/40 [00:00<00:02, 13.57it/s]"]},{"output_type":"stream","name":"stdout","text":["Epoch [5/200], Step [1/40], Loss D: 0.3484455645084381, Loss G: 0.7791671752929688\n"]},{"output_type":"stream","name":"stderr","text":["100%|██████████| 40/40 [00:03<00:00, 13.27it/s]\n","  5%|▌         | 2/40 [00:00<00:03, 11.83it/s]"]},{"output_type":"stream","name":"stdout","text":["Epoch [6/200], Step [1/40], Loss D: 0.3660849928855896, Loss G: 0.4376008212566376\n"]},{"output_type":"stream","name":"stderr","text":["100%|██████████| 40/40 [00:02<00:00, 13.57it/s]\n"," 10%|█         | 4/40 [00:00<00:02, 14.37it/s]"]},{"output_type":"stream","name":"stdout","text":["Epoch [7/200], Step [1/40], Loss D: 0.19903549551963806, Loss G: 0.4482547640800476\n"]},{"output_type":"stream","name":"stderr","text":["100%|██████████| 40/40 [00:02<00:00, 13.54it/s]\n","  8%|▊         | 3/40 [00:00<00:03, 11.50it/s]"]},{"output_type":"stream","name":"stdout","text":["Epoch [8/200], Step [1/40], Loss D: 0.2402799129486084, Loss G: 0.44940876960754395\n"]},{"output_type":"stream","name":"stderr","text":["100%|██████████| 40/40 [00:02<00:00, 14.34it/s]\n","  5%|▌         | 2/40 [00:00<00:02, 12.69it/s]"]},{"output_type":"stream","name":"stdout","text":["Epoch [9/200], Step [1/40], Loss D: 0.23075616359710693, Loss G: 0.39502328634262085\n"]},{"output_type":"stream","name":"stderr","text":["100%|██████████| 40/40 [00:02<00:00, 14.11it/s]\n","  5%|▌         | 2/40 [00:00<00:02, 12.81it/s]"]},{"output_type":"stream","name":"stdout","text":["Epoch [10/200], Step [1/40], Loss D: 0.20051191747188568, Loss G: 0.5690860748291016\n"]},{"output_type":"stream","name":"stderr","text":["100%|██████████| 40/40 [00:02<00:00, 14.87it/s]\n","  5%|▌         | 2/40 [00:00<00:02, 14.29it/s]"]},{"output_type":"stream","name":"stdout","text":["Epoch [11/200], Step [1/40], Loss D: 0.17834503948688507, Loss G: 0.5033254623413086\n"]},{"output_type":"stream","name":"stderr","text":["100%|██████████| 40/40 [00:02<00:00, 14.26it/s]\n","  5%|▌         | 2/40 [00:00<00:03, 12.32it/s]"]},{"output_type":"stream","name":"stdout","text":["Epoch [12/200], Step [1/40], Loss D: 0.16745591163635254, Loss G: 0.4606246054172516\n"]},{"output_type":"stream","name":"stderr","text":["100%|██████████| 40/40 [00:02<00:00, 13.86it/s]\n","  5%|▌         | 2/40 [00:00<00:02, 13.66it/s]"]},{"output_type":"stream","name":"stdout","text":["Epoch [13/200], Step [1/40], Loss D: 0.19081804156303406, Loss G: 0.4089992642402649\n"]},{"output_type":"stream","name":"stderr","text":["100%|██████████| 40/40 [00:02<00:00, 14.00it/s]\n","  5%|▌         | 2/40 [00:00<00:02, 13.75it/s]"]},{"output_type":"stream","name":"stdout","text":["Epoch [14/200], Step [1/40], Loss D: 0.23286303877830505, Loss G: 0.5637228488922119\n"]},{"output_type":"stream","name":"stderr","text":["100%|██████████| 40/40 [00:02<00:00, 14.12it/s]\n","  5%|▌         | 2/40 [00:00<00:02, 13.30it/s]"]},{"output_type":"stream","name":"stdout","text":["Epoch [15/200], Step [1/40], Loss D: 0.18862465023994446, Loss G: 0.49003392457962036\n"]},{"output_type":"stream","name":"stderr","text":["100%|██████████| 40/40 [00:02<00:00, 14.18it/s]\n","  5%|▌         | 2/40 [00:00<00:03, 11.73it/s]"]},{"output_type":"stream","name":"stdout","text":["Epoch [16/200], Step [1/40], Loss D: 0.16980309784412384, Loss G: 0.4718797206878662\n"]},{"output_type":"stream","name":"stderr","text":["100%|██████████| 40/40 [00:02<00:00, 13.63it/s]\n","  5%|▌         | 2/40 [00:00<00:03, 11.53it/s]"]},{"output_type":"stream","name":"stdout","text":["Epoch [17/200], Step [1/40], Loss D: 0.2787618935108185, Loss G: 0.422423392534256\n"]},{"output_type":"stream","name":"stderr","text":["100%|██████████| 40/40 [00:02<00:00, 13.90it/s]\n","  5%|▌         | 2/40 [00:00<00:02, 13.91it/s]"]},{"output_type":"stream","name":"stdout","text":["Epoch [18/200], Step [1/40], Loss D: 0.18895915150642395, Loss G: 0.6520817875862122\n"]},{"output_type":"stream","name":"stderr","text":["100%|██████████| 40/40 [00:02<00:00, 14.01it/s]\n","  5%|▌         | 2/40 [00:00<00:02, 14.57it/s]"]},{"output_type":"stream","name":"stdout","text":["Epoch [19/200], Step [1/40], Loss D: 0.20319393277168274, Loss G: 0.48101094365119934\n"]},{"output_type":"stream","name":"stderr","text":["100%|██████████| 40/40 [00:02<00:00, 14.20it/s]\n","  5%|▌         | 2/40 [00:00<00:02, 14.08it/s]"]},{"output_type":"stream","name":"stdout","text":["Epoch [20/200], Step [1/40], Loss D: 0.17087413370609283, Loss G: 0.42021864652633667\n"]},{"output_type":"stream","name":"stderr","text":["100%|██████████| 40/40 [00:02<00:00, 13.44it/s]\n","  5%|▌         | 2/40 [00:00<00:03, 12.34it/s]"]},{"output_type":"stream","name":"stdout","text":["Epoch [21/200], Step [1/40], Loss D: 0.215582937002182, Loss G: 0.6274607181549072\n"]},{"output_type":"stream","name":"stderr","text":["100%|██████████| 40/40 [00:02<00:00, 14.04it/s]\n","  5%|▌         | 2/40 [00:00<00:02, 12.90it/s]"]},{"output_type":"stream","name":"stdout","text":["Epoch [22/200], Step [1/40], Loss D: 0.2364180088043213, Loss G: 0.26874473690986633\n"]},{"output_type":"stream","name":"stderr","text":["100%|██████████| 40/40 [00:02<00:00, 14.24it/s]\n","  5%|▌         | 2/40 [00:00<00:03, 12.28it/s]"]},{"output_type":"stream","name":"stdout","text":["Epoch [23/200], Step [1/40], Loss D: 0.139405757188797, Loss G: 0.42882871627807617\n"]},{"output_type":"stream","name":"stderr","text":["100%|██████████| 40/40 [00:02<00:00, 14.72it/s]\n","  5%|▌         | 2/40 [00:00<00:03, 12.30it/s]"]},{"output_type":"stream","name":"stdout","text":["Epoch [24/200], Step [1/40], Loss D: 0.2446017563343048, Loss G: 0.7362380027770996\n"]},{"output_type":"stream","name":"stderr","text":["100%|██████████| 40/40 [00:02<00:00, 14.07it/s]\n","  5%|▌         | 2/40 [00:00<00:02, 13.46it/s]"]},{"output_type":"stream","name":"stdout","text":["Epoch [25/200], Step [1/40], Loss D: 0.14399190247058868, Loss G: 0.48243048787117004\n"]},{"output_type":"stream","name":"stderr","text":["100%|██████████| 40/40 [00:02<00:00, 13.94it/s]\n"," 10%|█         | 4/40 [00:00<00:02, 15.11it/s]"]},{"output_type":"stream","name":"stdout","text":["Epoch [26/200], Step [1/40], Loss D: 0.16803088784217834, Loss G: 0.48178714513778687\n"]},{"output_type":"stream","name":"stderr","text":["100%|██████████| 40/40 [00:02<00:00, 14.14it/s]\n","  5%|▌         | 2/40 [00:00<00:02, 15.68it/s]"]},{"output_type":"stream","name":"stdout","text":["Epoch [27/200], Step [1/40], Loss D: 0.15417052805423737, Loss G: 0.37170976400375366\n"]},{"output_type":"stream","name":"stderr","text":["100%|██████████| 40/40 [00:02<00:00, 14.30it/s]\n","  5%|▌         | 2/40 [00:00<00:02, 13.52it/s]"]},{"output_type":"stream","name":"stdout","text":["Epoch [28/200], Step [1/40], Loss D: 0.15996749699115753, Loss G: 0.6454449892044067\n"]},{"output_type":"stream","name":"stderr","text":["100%|██████████| 40/40 [00:02<00:00, 14.38it/s]\n","  5%|▌         | 2/40 [00:00<00:02, 12.70it/s]"]},{"output_type":"stream","name":"stdout","text":["Epoch [29/200], Step [1/40], Loss D: 0.29851433634757996, Loss G: 0.7373889684677124\n"]},{"output_type":"stream","name":"stderr","text":["100%|██████████| 40/40 [00:02<00:00, 13.50it/s]\n","  5%|▌         | 2/40 [00:00<00:02, 13.74it/s]"]},{"output_type":"stream","name":"stdout","text":["Epoch [30/200], Step [1/40], Loss D: 0.21070632338523865, Loss G: 0.6580139398574829\n"]},{"output_type":"stream","name":"stderr","text":["100%|██████████| 40/40 [00:02<00:00, 14.69it/s]\n","  5%|▌         | 2/40 [00:00<00:02, 12.72it/s]"]},{"output_type":"stream","name":"stdout","text":["Epoch [31/200], Step [1/40], Loss D: 0.18959692120552063, Loss G: 0.3824419379234314\n"]},{"output_type":"stream","name":"stderr","text":["100%|██████████| 40/40 [00:02<00:00, 14.14it/s]\n","  8%|▊         | 3/40 [00:00<00:02, 12.75it/s]"]},{"output_type":"stream","name":"stdout","text":["Epoch [32/200], Step [1/40], Loss D: 0.20519942045211792, Loss G: 0.3436957597732544\n"]},{"output_type":"stream","name":"stderr","text":["100%|██████████| 40/40 [00:02<00:00, 14.29it/s]\n"," 10%|█         | 4/40 [00:00<00:02, 14.79it/s]"]},{"output_type":"stream","name":"stdout","text":["Epoch [33/200], Step [1/40], Loss D: 0.4371354281902313, Loss G: 0.9008647203445435\n"]},{"output_type":"stream","name":"stderr","text":["100%|██████████| 40/40 [00:02<00:00, 13.42it/s]\n"," 10%|█         | 4/40 [00:00<00:02, 14.18it/s]"]},{"output_type":"stream","name":"stdout","text":["Epoch [34/200], Step [1/40], Loss D: 0.18006408214569092, Loss G: 0.7313816547393799\n"]},{"output_type":"stream","name":"stderr","text":["100%|██████████| 40/40 [00:02<00:00, 13.94it/s]\n","  5%|▌         | 2/40 [00:00<00:02, 14.38it/s]"]},{"output_type":"stream","name":"stdout","text":["Epoch [35/200], Step [1/40], Loss D: 0.1890825480222702, Loss G: 0.6544106006622314\n"]},{"output_type":"stream","name":"stderr","text":["100%|██████████| 40/40 [00:02<00:00, 14.29it/s]\n","  5%|▌         | 2/40 [00:00<00:02, 14.42it/s]"]},{"output_type":"stream","name":"stdout","text":["Epoch [36/200], Step [1/40], Loss D: 0.44091498851776123, Loss G: 0.48646825551986694\n"]},{"output_type":"stream","name":"stderr","text":["100%|██████████| 40/40 [00:02<00:00, 14.14it/s]\n","  5%|▌         | 2/40 [00:00<00:02, 13.46it/s]"]},{"output_type":"stream","name":"stdout","text":["Epoch [37/200], Step [1/40], Loss D: 0.09564442187547684, Loss G: 0.7750991582870483\n"]},{"output_type":"stream","name":"stderr","text":["100%|██████████| 40/40 [00:02<00:00, 14.38it/s]\n","  5%|▌         | 2/40 [00:00<00:02, 13.58it/s]"]},{"output_type":"stream","name":"stdout","text":["Epoch [38/200], Step [1/40], Loss D: 0.20959076285362244, Loss G: 0.4395776391029358\n"]},{"output_type":"stream","name":"stderr","text":["100%|██████████| 40/40 [00:02<00:00, 14.35it/s]\n","  5%|▌         | 2/40 [00:00<00:02, 13.97it/s]"]},{"output_type":"stream","name":"stdout","text":["Epoch [39/200], Step [1/40], Loss D: 0.24514976143836975, Loss G: 0.577376127243042\n"]},{"output_type":"stream","name":"stderr","text":["100%|██████████| 40/40 [00:02<00:00, 14.88it/s]\n"," 10%|█         | 4/40 [00:00<00:02, 14.55it/s]"]},{"output_type":"stream","name":"stdout","text":["Epoch [40/200], Step [1/40], Loss D: 0.1929669976234436, Loss G: 0.4141108989715576\n"]},{"output_type":"stream","name":"stderr","text":["100%|██████████| 40/40 [00:02<00:00, 14.48it/s]\n"," 10%|█         | 4/40 [00:00<00:02, 14.62it/s]"]},{"output_type":"stream","name":"stdout","text":["Epoch [41/200], Step [1/40], Loss D: 0.22956997156143188, Loss G: 0.31966811418533325\n"]},{"output_type":"stream","name":"stderr","text":["100%|██████████| 40/40 [00:02<00:00, 14.46it/s]\n","  5%|▌         | 2/40 [00:00<00:03, 11.44it/s]"]},{"output_type":"stream","name":"stdout","text":["Epoch [42/200], Step [1/40], Loss D: 0.10742618143558502, Loss G: 0.5953357815742493\n"]},{"output_type":"stream","name":"stderr","text":["100%|██████████| 40/40 [00:02<00:00, 13.58it/s]\n","  5%|▌         | 2/40 [00:00<00:02, 14.68it/s]"]},{"output_type":"stream","name":"stdout","text":["Epoch [43/200], Step [1/40], Loss D: 0.047361887991428375, Loss G: 0.7169003486633301\n"]},{"output_type":"stream","name":"stderr","text":["100%|██████████| 40/40 [00:02<00:00, 14.33it/s]\n","  5%|▌         | 2/40 [00:00<00:02, 14.55it/s]"]},{"output_type":"stream","name":"stdout","text":["Epoch [44/200], Step [1/40], Loss D: 0.5000730752944946, Loss G: 1.4210854715202004e-14\n"]},{"output_type":"stream","name":"stderr","text":["100%|██████████| 40/40 [00:02<00:00, 14.59it/s]\n","  5%|▌         | 2/40 [00:00<00:02, 13.10it/s]"]},{"output_type":"stream","name":"stdout","text":["Epoch [45/200], Step [1/40], Loss D: 0.5000319480895996, Loss G: 1.4210854715202004e-14\n"]},{"output_type":"stream","name":"stderr","text":["100%|██████████| 40/40 [00:02<00:00, 13.94it/s]\n","  5%|▌         | 2/40 [00:00<00:02, 14.15it/s]"]},{"output_type":"stream","name":"stdout","text":["Epoch [46/200], Step [1/40], Loss D: 0.5000068545341492, Loss G: 1.4210854715202004e-14\n"]},{"output_type":"stream","name":"stderr","text":["100%|██████████| 40/40 [00:02<00:00, 13.78it/s]\n","  5%|▌         | 2/40 [00:00<00:03, 11.84it/s]"]},{"output_type":"stream","name":"stdout","text":["Epoch [47/200], Step [1/40], Loss D: 0.5000011324882507, Loss G: 1.4210854715202004e-14\n"]},{"output_type":"stream","name":"stderr","text":["100%|██████████| 40/40 [00:02<00:00, 14.17it/s]\n","  5%|▌         | 2/40 [00:00<00:03, 12.62it/s]"]},{"output_type":"stream","name":"stdout","text":["Epoch [48/200], Step [1/40], Loss D: 0.5000083446502686, Loss G: 1.3322676295501878e-14\n"]},{"output_type":"stream","name":"stderr","text":["100%|██████████| 40/40 [00:02<00:00, 14.45it/s]\n"," 10%|█         | 4/40 [00:00<00:02, 14.17it/s]"]},{"output_type":"stream","name":"stdout","text":["Epoch [49/200], Step [1/40], Loss D: 0.500001072883606, Loss G: 1.4210854715202004e-14\n"]},{"output_type":"stream","name":"stderr","text":["100%|██████████| 40/40 [00:02<00:00, 13.86it/s]\n"," 10%|█         | 4/40 [00:00<00:02, 14.51it/s]"]},{"output_type":"stream","name":"stdout","text":["Epoch [50/200], Step [1/40], Loss D: 0.5000101923942566, Loss G: 1.4210854715202004e-14\n"]},{"output_type":"stream","name":"stderr","text":["100%|██████████| 40/40 [00:02<00:00, 13.42it/s]\n","  5%|▌         | 2/40 [00:00<00:02, 12.81it/s]"]},{"output_type":"stream","name":"stdout","text":["Epoch [51/200], Step [1/40], Loss D: 0.500011682510376, Loss G: 1.3322676295501878e-14\n"]},{"output_type":"stream","name":"stderr","text":["100%|██████████| 40/40 [00:02<00:00, 14.30it/s]\n","  5%|▌         | 2/40 [00:00<00:02, 14.00it/s]"]},{"output_type":"stream","name":"stdout","text":["Epoch [52/200], Step [1/40], Loss D: 0.5000040531158447, Loss G: 1.3322676295501878e-14\n"]},{"output_type":"stream","name":"stderr","text":["100%|██████████| 40/40 [00:02<00:00, 14.32it/s]\n","  5%|▌         | 2/40 [00:00<00:02, 15.37it/s]"]},{"output_type":"stream","name":"stdout","text":["Epoch [53/200], Step [1/40], Loss D: 0.5000078678131104, Loss G: 1.2434497875801753e-14\n"]},{"output_type":"stream","name":"stderr","text":["100%|██████████| 40/40 [00:02<00:00, 14.64it/s]\n","  5%|▌         | 2/40 [00:00<00:02, 13.40it/s]"]},{"output_type":"stream","name":"stdout","text":["Epoch [54/200], Step [1/40], Loss D: 0.5000091791152954, Loss G: 1.2434497875801753e-14\n"]},{"output_type":"stream","name":"stderr","text":["100%|██████████| 40/40 [00:02<00:00, 13.94it/s]\n","  5%|▌         | 2/40 [00:00<00:03, 12.05it/s]"]},{"output_type":"stream","name":"stdout","text":["Epoch [55/200], Step [1/40], Loss D: 0.5000014305114746, Loss G: 1.2434497875801753e-14\n"]},{"output_type":"stream","name":"stderr","text":["100%|██████████| 40/40 [00:02<00:00, 13.44it/s]\n","  5%|▌         | 2/40 [00:00<00:02, 12.69it/s]"]},{"output_type":"stream","name":"stdout","text":["Epoch [56/200], Step [1/40], Loss D: 0.5000098943710327, Loss G: 1.0658141036401503e-14\n"]},{"output_type":"stream","name":"stderr","text":["100%|██████████| 40/40 [00:02<00:00, 14.00it/s]\n","  5%|▌         | 2/40 [00:00<00:02, 12.71it/s]"]},{"output_type":"stream","name":"stdout","text":["Epoch [57/200], Step [1/40], Loss D: 0.5000013113021851, Loss G: 1.0658141036401503e-14\n"]},{"output_type":"stream","name":"stderr","text":["100%|██████████| 40/40 [00:02<00:00, 13.96it/s]\n","  5%|▌         | 2/40 [00:00<00:02, 13.07it/s]"]},{"output_type":"stream","name":"stdout","text":["Epoch [58/200], Step [1/40], Loss D: 0.5000008940696716, Loss G: 5.329070518200751e-15\n"]},{"output_type":"stream","name":"stderr","text":["100%|██████████| 40/40 [00:02<00:00, 14.42it/s]\n","  5%|▌         | 2/40 [00:00<00:03, 12.03it/s]"]},{"output_type":"stream","name":"stdout","text":["Epoch [59/200], Step [1/40], Loss D: 0.5000002980232239, Loss G: 6.217248937900877e-15\n"]},{"output_type":"stream","name":"stderr","text":["100%|██████████| 40/40 [00:03<00:00, 13.32it/s]\n","  5%|▌         | 2/40 [00:00<00:02, 12.94it/s]"]},{"output_type":"stream","name":"stdout","text":["Epoch [60/200], Step [1/40], Loss D: 0.5000017285346985, Loss G: 9.769962616701378e-15\n"]},{"output_type":"stream","name":"stderr","text":["100%|██████████| 40/40 [00:02<00:00, 14.11it/s]\n","  5%|▌         | 2/40 [00:00<00:02, 13.23it/s]"]},{"output_type":"stream","name":"stdout","text":["Epoch [61/200], Step [1/40], Loss D: 0.5000025033950806, Loss G: 6.217248937900877e-15\n"]},{"output_type":"stream","name":"stderr","text":["100%|██████████| 40/40 [00:02<00:00, 14.16it/s]\n","  5%|▌         | 2/40 [00:00<00:02, 14.04it/s]"]},{"output_type":"stream","name":"stdout","text":["Epoch [62/200], Step [1/40], Loss D: 0.5000059008598328, Loss G: 6.217248937900877e-15\n"]},{"output_type":"stream","name":"stderr","text":["100%|██████████| 40/40 [00:02<00:00, 14.35it/s]\n","  5%|▌         | 2/40 [00:00<00:02, 15.05it/s]"]},{"output_type":"stream","name":"stdout","text":["Epoch [63/200], Step [1/40], Loss D: 0.5000059604644775, Loss G: 6.217248937900877e-15\n"]},{"output_type":"stream","name":"stderr","text":["100%|██████████| 40/40 [00:02<00:00, 13.51it/s]\n","  5%|▌         | 2/40 [00:00<00:03, 11.48it/s]"]},{"output_type":"stream","name":"stdout","text":["Epoch [64/200], Step [1/40], Loss D: 0.5000118613243103, Loss G: 4.440892098500626e-15\n"]},{"output_type":"stream","name":"stderr","text":["100%|██████████| 40/40 [00:02<00:00, 13.54it/s]\n","  5%|▌         | 2/40 [00:00<00:02, 13.13it/s]"]},{"output_type":"stream","name":"stdout","text":["Epoch [65/200], Step [1/40], Loss D: 0.5000004172325134, Loss G: 8.881784197001252e-16\n"]},{"output_type":"stream","name":"stderr","text":["100%|██████████| 40/40 [00:02<00:00, 14.58it/s]\n"," 10%|█         | 4/40 [00:00<00:02, 14.21it/s]"]},{"output_type":"stream","name":"stdout","text":["Epoch [66/200], Step [1/40], Loss D: 0.5000012516975403, Loss G: 2.6645352591003757e-15\n"]},{"output_type":"stream","name":"stderr","text":["100%|██████████| 40/40 [00:02<00:00, 14.77it/s]\n"," 10%|█         | 4/40 [00:00<00:02, 14.50it/s]"]},{"output_type":"stream","name":"stdout","text":["Epoch [67/200], Step [1/40], Loss D: 0.500002920627594, Loss G: 3.552713678800501e-15\n"]},{"output_type":"stream","name":"stderr","text":["100%|██████████| 40/40 [00:02<00:00, 14.52it/s]\n","  5%|▌         | 2/40 [00:00<00:02, 14.22it/s]"]},{"output_type":"stream","name":"stdout","text":["Epoch [68/200], Step [1/40], Loss D: 0.5000015497207642, Loss G: 1.7763568394002505e-15\n"]},{"output_type":"stream","name":"stderr","text":["100%|██████████| 40/40 [00:02<00:00, 13.82it/s]\n","  5%|▌         | 2/40 [00:00<00:02, 13.51it/s]"]},{"output_type":"stream","name":"stdout","text":["Epoch [69/200], Step [1/40], Loss D: 0.5000051259994507, Loss G: 1.7763568394002505e-15\n"]},{"output_type":"stream","name":"stderr","text":["100%|██████████| 40/40 [00:02<00:00, 14.88it/s]\n","  5%|▌         | 2/40 [00:00<00:02, 15.26it/s]"]},{"output_type":"stream","name":"stdout","text":["Epoch [70/200], Step [1/40], Loss D: 0.5000014305114746, Loss G: 3.552713678800501e-15\n"]},{"output_type":"stream","name":"stderr","text":["100%|██████████| 40/40 [00:02<00:00, 14.67it/s]\n","  5%|▌         | 2/40 [00:00<00:02, 14.15it/s]"]},{"output_type":"stream","name":"stdout","text":["Epoch [71/200], Step [1/40], Loss D: 0.5000010132789612, Loss G: 8.881784197001252e-16\n"]},{"output_type":"stream","name":"stderr","text":["100%|██████████| 40/40 [00:02<00:00, 14.08it/s]\n","  5%|▌         | 2/40 [00:00<00:03, 12.51it/s]"]},{"output_type":"stream","name":"stdout","text":["Epoch [72/200], Step [1/40], Loss D: 0.5000006556510925, Loss G: 0.0\n"]},{"output_type":"stream","name":"stderr","text":["100%|██████████| 40/40 [00:02<00:00, 13.36it/s]\n","  5%|▌         | 2/40 [00:00<00:03, 11.12it/s]"]},{"output_type":"stream","name":"stdout","text":["Epoch [73/200], Step [1/40], Loss D: 0.5000020861625671, Loss G: 1.7763568394002505e-15\n"]},{"output_type":"stream","name":"stderr","text":["100%|██████████| 40/40 [00:02<00:00, 14.27it/s]\n","  5%|▌         | 2/40 [00:00<00:03, 12.51it/s]"]},{"output_type":"stream","name":"stdout","text":["Epoch [74/200], Step [1/40], Loss D: 0.5000019669532776, Loss G: 8.881784197001252e-16\n"]},{"output_type":"stream","name":"stderr","text":["100%|██████████| 40/40 [00:02<00:00, 14.41it/s]\n","  5%|▌         | 2/40 [00:00<00:03, 12.54it/s]"]},{"output_type":"stream","name":"stdout","text":["Epoch [75/200], Step [1/40], Loss D: 0.500002384185791, Loss G: 8.881784197001252e-16\n"]},{"output_type":"stream","name":"stderr","text":["100%|██████████| 40/40 [00:02<00:00, 13.80it/s]\n"," 10%|█         | 4/40 [00:00<00:02, 14.26it/s]"]},{"output_type":"stream","name":"stdout","text":["Epoch [76/200], Step [1/40], Loss D: 0.5000026822090149, Loss G: 8.881784197001252e-16\n"]},{"output_type":"stream","name":"stderr","text":["100%|██████████| 40/40 [00:02<00:00, 13.86it/s]\n","  8%|▊         | 3/40 [00:00<00:02, 12.87it/s]"]},{"output_type":"stream","name":"stdout","text":["Epoch [77/200], Step [1/40], Loss D: 0.5000022649765015, Loss G: 0.0\n"]},{"output_type":"stream","name":"stderr","text":["100%|██████████| 40/40 [00:03<00:00, 13.05it/s]\n","  5%|▌         | 2/40 [00:00<00:03, 12.43it/s]"]},{"output_type":"stream","name":"stdout","text":["Epoch [78/200], Step [1/40], Loss D: 0.5000008940696716, Loss G: 0.0\n"]},{"output_type":"stream","name":"stderr","text":["100%|██████████| 40/40 [00:02<00:00, 13.53it/s]\n","  5%|▌         | 2/40 [00:00<00:02, 13.77it/s]"]},{"output_type":"stream","name":"stdout","text":["Epoch [79/200], Step [1/40], Loss D: 0.5000025629997253, Loss G: 0.0\n"]},{"output_type":"stream","name":"stderr","text":["100%|██████████| 40/40 [00:02<00:00, 14.54it/s]\n","  5%|▌         | 2/40 [00:00<00:02, 14.36it/s]"]},{"output_type":"stream","name":"stdout","text":["Epoch [80/200], Step [1/40], Loss D: 0.5000013113021851, Loss G: 0.0\n"]},{"output_type":"stream","name":"stderr","text":["100%|██████████| 40/40 [00:02<00:00, 14.08it/s]\n","  5%|▌         | 2/40 [00:00<00:03, 12.06it/s]"]},{"output_type":"stream","name":"stdout","text":["Epoch [81/200], Step [1/40], Loss D: 0.500000536441803, Loss G: 8.881784197001252e-16\n"]},{"output_type":"stream","name":"stderr","text":["100%|██████████| 40/40 [00:03<00:00, 13.24it/s]\n","  5%|▌         | 2/40 [00:00<00:02, 12.84it/s]"]},{"output_type":"stream","name":"stdout","text":["Epoch [82/200], Step [1/40], Loss D: 0.5000011324882507, Loss G: 0.0\n"]},{"output_type":"stream","name":"stderr","text":["100%|██████████| 40/40 [00:02<00:00, 14.19it/s]\n","  5%|▌         | 2/40 [00:00<00:03, 12.17it/s]"]},{"output_type":"stream","name":"stdout","text":["Epoch [83/200], Step [1/40], Loss D: 0.5000012516975403, Loss G: 0.0\n"]},{"output_type":"stream","name":"stderr","text":["100%|██████████| 40/40 [00:02<00:00, 14.24it/s]\n","  5%|▌         | 2/40 [00:00<00:02, 13.08it/s]"]},{"output_type":"stream","name":"stdout","text":["Epoch [84/200], Step [1/40], Loss D: 0.5000004768371582, Loss G: 0.0\n"]},{"output_type":"stream","name":"stderr","text":["100%|██████████| 40/40 [00:02<00:00, 14.37it/s]\n","  5%|▌         | 2/40 [00:00<00:03, 12.62it/s]"]},{"output_type":"stream","name":"stdout","text":["Epoch [85/200], Step [1/40], Loss D: 0.500001847743988, Loss G: 0.0\n"]},{"output_type":"stream","name":"stderr","text":["100%|██████████| 40/40 [00:03<00:00, 13.04it/s]\n","  5%|▌         | 2/40 [00:00<00:02, 13.50it/s]"]},{"output_type":"stream","name":"stdout","text":["Epoch [86/200], Step [1/40], Loss D: 0.5000012516975403, Loss G: 0.0\n"]},{"output_type":"stream","name":"stderr","text":["100%|██████████| 40/40 [00:02<00:00, 13.63it/s]\n","  5%|▌         | 2/40 [00:00<00:03, 11.74it/s]"]},{"output_type":"stream","name":"stdout","text":["Epoch [87/200], Step [1/40], Loss D: 0.5000016689300537, Loss G: 0.0\n"]},{"output_type":"stream","name":"stderr","text":["100%|██████████| 40/40 [00:02<00:00, 13.97it/s]\n","  5%|▌         | 2/40 [00:00<00:02, 14.05it/s]"]},{"output_type":"stream","name":"stdout","text":["Epoch [88/200], Step [1/40], Loss D: 0.5000003576278687, Loss G: 0.0\n"]},{"output_type":"stream","name":"stderr","text":["100%|██████████| 40/40 [00:02<00:00, 14.44it/s]\n","  5%|▌         | 2/40 [00:00<00:03, 12.58it/s]"]},{"output_type":"stream","name":"stdout","text":["Epoch [89/200], Step [1/40], Loss D: 0.500001072883606, Loss G: 0.0\n"]},{"output_type":"stream","name":"stderr","text":["100%|██████████| 40/40 [00:02<00:00, 13.74it/s]\n","  5%|▌         | 2/40 [00:00<00:02, 12.78it/s]"]},{"output_type":"stream","name":"stdout","text":["Epoch [90/200], Step [1/40], Loss D: 0.5000004768371582, Loss G: 0.0\n"]},{"output_type":"stream","name":"stderr","text":["100%|██████████| 40/40 [00:02<00:00, 13.63it/s]\n","  5%|▌         | 2/40 [00:00<00:02, 14.71it/s]"]},{"output_type":"stream","name":"stdout","text":["Epoch [91/200], Step [1/40], Loss D: 0.5000006556510925, Loss G: 0.0\n"]},{"output_type":"stream","name":"stderr","text":["100%|██████████| 40/40 [00:02<00:00, 14.60it/s]\n","  5%|▌         | 2/40 [00:00<00:03, 12.15it/s]"]},{"output_type":"stream","name":"stdout","text":["Epoch [92/200], Step [1/40], Loss D: 0.5000036954879761, Loss G: 0.0\n"]},{"output_type":"stream","name":"stderr","text":["100%|██████████| 40/40 [00:02<00:00, 14.19it/s]\n","  5%|▌         | 2/40 [00:00<00:02, 13.48it/s]"]},{"output_type":"stream","name":"stdout","text":["Epoch [93/200], Step [1/40], Loss D: 0.5000002980232239, Loss G: 0.0\n"]},{"output_type":"stream","name":"stderr","text":["100%|██████████| 40/40 [00:02<00:00, 13.95it/s]\n","  8%|▊         | 3/40 [00:00<00:02, 12.48it/s]"]},{"output_type":"stream","name":"stdout","text":["Epoch [94/200], Step [1/40], Loss D: 0.5000009536743164, Loss G: 0.0\n"]},{"output_type":"stream","name":"stderr","text":["100%|██████████| 40/40 [00:02<00:00, 14.07it/s]\n"," 10%|█         | 4/40 [00:00<00:02, 14.09it/s]"]},{"output_type":"stream","name":"stdout","text":["Epoch [95/200], Step [1/40], Loss D: 0.5000008344650269, Loss G: 0.0\n"]},{"output_type":"stream","name":"stderr","text":["100%|██████████| 40/40 [00:02<00:00, 14.61it/s]\n"," 10%|█         | 4/40 [00:00<00:02, 16.14it/s]"]},{"output_type":"stream","name":"stdout","text":["Epoch [96/200], Step [1/40], Loss D: 0.5000005960464478, Loss G: 0.0\n"]},{"output_type":"stream","name":"stderr","text":["100%|██████████| 40/40 [00:02<00:00, 15.01it/s]\n","  5%|▌         | 2/40 [00:00<00:03, 12.22it/s]"]},{"output_type":"stream","name":"stdout","text":["Epoch [97/200], Step [1/40], Loss D: 0.5000004768371582, Loss G: 0.0\n"]},{"output_type":"stream","name":"stderr","text":["100%|██████████| 40/40 [00:02<00:00, 14.52it/s]\n","  5%|▌         | 2/40 [00:00<00:02, 13.33it/s]"]},{"output_type":"stream","name":"stdout","text":["Epoch [98/200], Step [1/40], Loss D: 0.5000005960464478, Loss G: 0.0\n"]},{"output_type":"stream","name":"stderr","text":["100%|██████████| 40/40 [00:02<00:00, 13.55it/s]\n","  5%|▌         | 2/40 [00:00<00:02, 13.43it/s]"]},{"output_type":"stream","name":"stdout","text":["Epoch [99/200], Step [1/40], Loss D: 0.5000002980232239, Loss G: 0.0\n"]},{"output_type":"stream","name":"stderr","text":["100%|██████████| 40/40 [00:02<00:00, 13.87it/s]\n","  5%|▌         | 2/40 [00:00<00:02, 12.98it/s]"]},{"output_type":"stream","name":"stdout","text":["Epoch [100/200], Step [1/40], Loss D: 0.5000007152557373, Loss G: 0.0\n"]},{"output_type":"stream","name":"stderr","text":["100%|██████████| 40/40 [00:02<00:00, 14.65it/s]\n","  5%|▌         | 2/40 [00:00<00:02, 14.00it/s]"]},{"output_type":"stream","name":"stdout","text":["Epoch [101/200], Step [1/40], Loss D: 0.5000007748603821, Loss G: 0.0\n"]},{"output_type":"stream","name":"stderr","text":["100%|██████████| 40/40 [00:02<00:00, 14.51it/s]\n"," 10%|█         | 4/40 [00:00<00:02, 13.86it/s]"]},{"output_type":"stream","name":"stdout","text":["Epoch [102/200], Step [1/40], Loss D: 0.5000009536743164, Loss G: 0.0\n"]},{"output_type":"stream","name":"stderr","text":["100%|██████████| 40/40 [00:02<00:00, 13.83it/s]\n","  5%|▌         | 2/40 [00:00<00:03, 11.24it/s]"]},{"output_type":"stream","name":"stdout","text":["Epoch [103/200], Step [1/40], Loss D: 0.5000004172325134, Loss G: 0.0\n"]},{"output_type":"stream","name":"stderr","text":["100%|██████████| 40/40 [00:02<00:00, 13.81it/s]\n","  5%|▌         | 2/40 [00:00<00:02, 13.60it/s]"]},{"output_type":"stream","name":"stdout","text":["Epoch [104/200], Step [1/40], Loss D: 0.5000002980232239, Loss G: 0.0\n"]},{"output_type":"stream","name":"stderr","text":["100%|██████████| 40/40 [00:02<00:00, 13.95it/s]\n","  5%|▌         | 2/40 [00:00<00:02, 13.71it/s]"]},{"output_type":"stream","name":"stdout","text":["Epoch [105/200], Step [1/40], Loss D: 0.5000004768371582, Loss G: 0.0\n"]},{"output_type":"stream","name":"stderr","text":["100%|██████████| 40/40 [00:02<00:00, 14.40it/s]\n","  5%|▌         | 2/40 [00:00<00:02, 14.12it/s]"]},{"output_type":"stream","name":"stdout","text":["Epoch [106/200], Step [1/40], Loss D: 0.5000017285346985, Loss G: 0.0\n"]},{"output_type":"stream","name":"stderr","text":["100%|██████████| 40/40 [00:02<00:00, 14.03it/s]\n","  5%|▌         | 2/40 [00:00<00:02, 14.24it/s]"]},{"output_type":"stream","name":"stdout","text":["Epoch [107/200], Step [1/40], Loss D: 0.500001072883606, Loss G: 0.0\n"]},{"output_type":"stream","name":"stderr","text":["100%|██████████| 40/40 [00:02<00:00, 14.03it/s]\n","  5%|▌         | 2/40 [00:00<00:02, 13.59it/s]"]},{"output_type":"stream","name":"stdout","text":["Epoch [108/200], Step [1/40], Loss D: 0.5000002384185791, Loss G: 0.0\n"]},{"output_type":"stream","name":"stderr","text":["100%|██████████| 40/40 [00:02<00:00, 14.25it/s]\n","  5%|▌         | 2/40 [00:00<00:02, 14.38it/s]"]},{"output_type":"stream","name":"stdout","text":["Epoch [109/200], Step [1/40], Loss D: 0.5000008940696716, Loss G: 0.0\n"]},{"output_type":"stream","name":"stderr","text":["100%|██████████| 40/40 [00:02<00:00, 14.52it/s]\n","  5%|▌         | 2/40 [00:00<00:02, 12.87it/s]"]},{"output_type":"stream","name":"stdout","text":["Epoch [110/200], Step [1/40], Loss D: 0.5000004768371582, Loss G: 0.0\n"]},{"output_type":"stream","name":"stderr","text":["100%|██████████| 40/40 [00:02<00:00, 14.76it/s]\n","  5%|▌         | 2/40 [00:00<00:02, 13.24it/s]"]},{"output_type":"stream","name":"stdout","text":["Epoch [111/200], Step [1/40], Loss D: 0.5000004172325134, Loss G: 0.0\n"]},{"output_type":"stream","name":"stderr","text":["100%|██████████| 40/40 [00:02<00:00, 13.47it/s]\n"," 10%|█         | 4/40 [00:00<00:02, 14.09it/s]"]},{"output_type":"stream","name":"stdout","text":["Epoch [112/200], Step [1/40], Loss D: 0.5000013709068298, Loss G: 0.0\n"]},{"output_type":"stream","name":"stderr","text":["100%|██████████| 40/40 [00:02<00:00, 13.85it/s]\n","  5%|▌         | 2/40 [00:00<00:02, 13.33it/s]"]},{"output_type":"stream","name":"stdout","text":["Epoch [113/200], Step [1/40], Loss D: 0.5000002980232239, Loss G: 0.0\n"]},{"output_type":"stream","name":"stderr","text":["100%|██████████| 40/40 [00:02<00:00, 13.55it/s]\n","  5%|▌         | 2/40 [00:00<00:02, 13.83it/s]"]},{"output_type":"stream","name":"stdout","text":["Epoch [114/200], Step [1/40], Loss D: 0.5000008940696716, Loss G: 0.0\n"]},{"output_type":"stream","name":"stderr","text":["100%|██████████| 40/40 [00:02<00:00, 13.94it/s]\n","  5%|▌         | 2/40 [00:00<00:03, 12.11it/s]"]},{"output_type":"stream","name":"stdout","text":["Epoch [115/200], Step [1/40], Loss D: 0.5000002384185791, Loss G: 0.0\n"]},{"output_type":"stream","name":"stderr","text":["100%|██████████| 40/40 [00:02<00:00, 13.52it/s]\n","  5%|▌         | 2/40 [00:00<00:03, 12.48it/s]"]},{"output_type":"stream","name":"stdout","text":["Epoch [116/200], Step [1/40], Loss D: 0.5000002980232239, Loss G: 0.0\n"]},{"output_type":"stream","name":"stderr","text":["100%|██████████| 40/40 [00:02<00:00, 13.69it/s]\n","  5%|▌         | 2/40 [00:00<00:02, 13.30it/s]"]},{"output_type":"stream","name":"stdout","text":["Epoch [117/200], Step [1/40], Loss D: 0.5000007748603821, Loss G: 0.0\n"]},{"output_type":"stream","name":"stderr","text":["100%|██████████| 40/40 [00:02<00:00, 14.11it/s]\n"," 10%|█         | 4/40 [00:00<00:02, 15.02it/s]"]},{"output_type":"stream","name":"stdout","text":["Epoch [118/200], Step [1/40], Loss D: 0.5000001788139343, Loss G: 0.0\n"]},{"output_type":"stream","name":"stderr","text":["100%|██████████| 40/40 [00:02<00:00, 14.27it/s]\n","  5%|▌         | 2/40 [00:00<00:03, 12.65it/s]"]},{"output_type":"stream","name":"stdout","text":["Epoch [119/200], Step [1/40], Loss D: 0.5000002384185791, Loss G: 0.0\n"]},{"output_type":"stream","name":"stderr","text":["100%|██████████| 40/40 [00:02<00:00, 14.13it/s]\n","  5%|▌         | 2/40 [00:00<00:03, 11.43it/s]"]},{"output_type":"stream","name":"stdout","text":["Epoch [120/200], Step [1/40], Loss D: 0.5000006556510925, Loss G: 0.0\n"]},{"output_type":"stream","name":"stderr","text":["100%|██████████| 40/40 [00:02<00:00, 13.66it/s]\n","  5%|▌         | 2/40 [00:00<00:02, 13.80it/s]"]},{"output_type":"stream","name":"stdout","text":["Epoch [121/200], Step [1/40], Loss D: 0.5000002384185791, Loss G: 0.0\n"]},{"output_type":"stream","name":"stderr","text":["100%|██████████| 40/40 [00:02<00:00, 14.14it/s]\n","  5%|▌         | 2/40 [00:00<00:02, 14.28it/s]"]},{"output_type":"stream","name":"stdout","text":["Epoch [122/200], Step [1/40], Loss D: 0.5000008940696716, Loss G: 0.0\n"]},{"output_type":"stream","name":"stderr","text":["100%|██████████| 40/40 [00:02<00:00, 14.65it/s]\n","  5%|▌         | 2/40 [00:00<00:02, 12.71it/s]"]},{"output_type":"stream","name":"stdout","text":["Epoch [123/200], Step [1/40], Loss D: 0.5000001788139343, Loss G: 0.0\n"]},{"output_type":"stream","name":"stderr","text":["100%|██████████| 40/40 [00:02<00:00, 14.23it/s]\n","  5%|▌         | 2/40 [00:00<00:02, 14.94it/s]"]},{"output_type":"stream","name":"stdout","text":["Epoch [124/200], Step [1/40], Loss D: 0.5000007748603821, Loss G: 0.0\n"]},{"output_type":"stream","name":"stderr","text":["100%|██████████| 40/40 [00:02<00:00, 14.05it/s]\n","  5%|▌         | 2/40 [00:00<00:02, 13.65it/s]"]},{"output_type":"stream","name":"stdout","text":["Epoch [125/200], Step [1/40], Loss D: 0.500000536441803, Loss G: 0.0\n"]},{"output_type":"stream","name":"stderr","text":["100%|██████████| 40/40 [00:02<00:00, 14.86it/s]\n","  5%|▌         | 2/40 [00:00<00:02, 12.70it/s]"]},{"output_type":"stream","name":"stdout","text":["Epoch [126/200], Step [1/40], Loss D: 0.5000014305114746, Loss G: 0.0\n"]},{"output_type":"stream","name":"stderr","text":["100%|██████████| 40/40 [00:02<00:00, 14.48it/s]\n","  5%|▌         | 2/40 [00:00<00:03, 12.47it/s]"]},{"output_type":"stream","name":"stdout","text":["Epoch [127/200], Step [1/40], Loss D: 0.5000004172325134, Loss G: 0.0\n"]},{"output_type":"stream","name":"stderr","text":["100%|██████████| 40/40 [00:02<00:00, 14.43it/s]\n","  5%|▌         | 2/40 [00:00<00:02, 13.10it/s]"]},{"output_type":"stream","name":"stdout","text":["Epoch [128/200], Step [1/40], Loss D: 0.5000002384185791, Loss G: 0.0\n"]},{"output_type":"stream","name":"stderr","text":["100%|██████████| 40/40 [00:02<00:00, 13.68it/s]\n","  5%|▌         | 2/40 [00:00<00:02, 12.90it/s]"]},{"output_type":"stream","name":"stdout","text":["Epoch [129/200], Step [1/40], Loss D: 0.5000004172325134, Loss G: 0.0\n"]},{"output_type":"stream","name":"stderr","text":["100%|██████████| 40/40 [00:02<00:00, 13.92it/s]\n","  5%|▌         | 2/40 [00:00<00:02, 14.61it/s]"]},{"output_type":"stream","name":"stdout","text":["Epoch [130/200], Step [1/40], Loss D: 0.5000001788139343, Loss G: 0.0\n"]},{"output_type":"stream","name":"stderr","text":["100%|██████████| 40/40 [00:02<00:00, 14.69it/s]\n","  5%|▌         | 2/40 [00:00<00:02, 14.35it/s]"]},{"output_type":"stream","name":"stdout","text":["Epoch [131/200], Step [1/40], Loss D: 0.5000003576278687, Loss G: 0.0\n"]},{"output_type":"stream","name":"stderr","text":["100%|██████████| 40/40 [00:02<00:00, 14.47it/s]\n","  5%|▌         | 2/40 [00:00<00:02, 14.04it/s]"]},{"output_type":"stream","name":"stdout","text":["Epoch [132/200], Step [1/40], Loss D: 0.5000004172325134, Loss G: 0.0\n"]},{"output_type":"stream","name":"stderr","text":["100%|██████████| 40/40 [00:02<00:00, 14.00it/s]\n","  5%|▌         | 2/40 [00:00<00:02, 13.38it/s]"]},{"output_type":"stream","name":"stdout","text":["Epoch [133/200], Step [1/40], Loss D: 0.5000007152557373, Loss G: 0.0\n"]},{"output_type":"stream","name":"stderr","text":["100%|██████████| 40/40 [00:02<00:00, 13.66it/s]\n","  5%|▌         | 2/40 [00:00<00:03, 12.25it/s]"]},{"output_type":"stream","name":"stdout","text":["Epoch [134/200], Step [1/40], Loss D: 0.5000002980232239, Loss G: 0.0\n"]},{"output_type":"stream","name":"stderr","text":["100%|██████████| 40/40 [00:02<00:00, 14.19it/s]\n","  5%|▌         | 2/40 [00:00<00:02, 12.80it/s]"]},{"output_type":"stream","name":"stdout","text":["Epoch [135/200], Step [1/40], Loss D: 0.5000002980232239, Loss G: 0.0\n"]},{"output_type":"stream","name":"stderr","text":["100%|██████████| 40/40 [00:02<00:00, 14.01it/s]\n"," 10%|█         | 4/40 [00:00<00:02, 13.76it/s]"]},{"output_type":"stream","name":"stdout","text":["Epoch [136/200], Step [1/40], Loss D: 0.5000003576278687, Loss G: 0.0\n"]},{"output_type":"stream","name":"stderr","text":["100%|██████████| 40/40 [00:02<00:00, 14.16it/s]\n","  5%|▌         | 2/40 [00:00<00:02, 13.45it/s]"]},{"output_type":"stream","name":"stdout","text":["Epoch [137/200], Step [1/40], Loss D: 0.5000005960464478, Loss G: 0.0\n"]},{"output_type":"stream","name":"stderr","text":["100%|██████████| 40/40 [00:02<00:00, 13.79it/s]\n","  5%|▌         | 2/40 [00:00<00:02, 14.66it/s]"]},{"output_type":"stream","name":"stdout","text":["Epoch [138/200], Step [1/40], Loss D: 0.5000000596046448, Loss G: 0.0\n"]},{"output_type":"stream","name":"stderr","text":["100%|██████████| 40/40 [00:02<00:00, 14.13it/s]\n","  5%|▌         | 2/40 [00:00<00:02, 13.77it/s]"]},{"output_type":"stream","name":"stdout","text":["Epoch [139/200], Step [1/40], Loss D: 0.5000001192092896, Loss G: 0.0\n"]},{"output_type":"stream","name":"stderr","text":["100%|██████████| 40/40 [00:02<00:00, 14.29it/s]\n","  5%|▌         | 2/40 [00:00<00:02, 13.02it/s]"]},{"output_type":"stream","name":"stdout","text":["Epoch [140/200], Step [1/40], Loss D: 0.5000001788139343, Loss G: 0.0\n"]},{"output_type":"stream","name":"stderr","text":["100%|██████████| 40/40 [00:02<00:00, 13.69it/s]\n","  5%|▌         | 2/40 [00:00<00:02, 13.47it/s]"]},{"output_type":"stream","name":"stdout","text":["Epoch [141/200], Step [1/40], Loss D: 0.5000007748603821, Loss G: 0.0\n"]},{"output_type":"stream","name":"stderr","text":["100%|██████████| 40/40 [00:02<00:00, 13.46it/s]\n","  5%|▌         | 2/40 [00:00<00:03, 11.49it/s]"]},{"output_type":"stream","name":"stdout","text":["Epoch [142/200], Step [1/40], Loss D: 0.5000004768371582, Loss G: 0.0\n"]},{"output_type":"stream","name":"stderr","text":["100%|██████████| 40/40 [00:02<00:00, 13.97it/s]\n","  5%|▌         | 2/40 [00:00<00:03, 11.56it/s]"]},{"output_type":"stream","name":"stdout","text":["Epoch [143/200], Step [1/40], Loss D: 0.5000000596046448, Loss G: 0.0\n"]},{"output_type":"stream","name":"stderr","text":["100%|██████████| 40/40 [00:02<00:00, 14.09it/s]\n","  5%|▌         | 2/40 [00:00<00:02, 13.41it/s]"]},{"output_type":"stream","name":"stdout","text":["Epoch [144/200], Step [1/40], Loss D: 0.5000004768371582, Loss G: 0.0\n"]},{"output_type":"stream","name":"stderr","text":["100%|██████████| 40/40 [00:02<00:00, 14.18it/s]\n","  5%|▌         | 2/40 [00:00<00:02, 13.43it/s]"]},{"output_type":"stream","name":"stdout","text":["Epoch [145/200], Step [1/40], Loss D: 0.5000002980232239, Loss G: 0.0\n"]},{"output_type":"stream","name":"stderr","text":["100%|██████████| 40/40 [00:02<00:00, 14.00it/s]\n","  5%|▌         | 2/40 [00:00<00:02, 12.69it/s]"]},{"output_type":"stream","name":"stdout","text":["Epoch [146/200], Step [1/40], Loss D: 0.5000000596046448, Loss G: 0.0\n"]},{"output_type":"stream","name":"stderr","text":["100%|██████████| 40/40 [00:02<00:00, 13.85it/s]\n","  5%|▌         | 2/40 [00:00<00:02, 12.91it/s]"]},{"output_type":"stream","name":"stdout","text":["Epoch [147/200], Step [1/40], Loss D: 0.5000003576278687, Loss G: 0.0\n"]},{"output_type":"stream","name":"stderr","text":["100%|██████████| 40/40 [00:02<00:00, 14.10it/s]\n","  5%|▌         | 2/40 [00:00<00:03, 11.73it/s]"]},{"output_type":"stream","name":"stdout","text":["Epoch [148/200], Step [1/40], Loss D: 0.5000002384185791, Loss G: 0.0\n"]},{"output_type":"stream","name":"stderr","text":["100%|██████████| 40/40 [00:02<00:00, 14.11it/s]\n","  5%|▌         | 2/40 [00:00<00:03, 11.93it/s]"]},{"output_type":"stream","name":"stdout","text":["Epoch [149/200], Step [1/40], Loss D: 0.5000001788139343, Loss G: 0.0\n"]},{"output_type":"stream","name":"stderr","text":["100%|██████████| 40/40 [00:02<00:00, 13.87it/s]\n","  5%|▌         | 2/40 [00:00<00:03, 12.49it/s]"]},{"output_type":"stream","name":"stdout","text":["Epoch [150/200], Step [1/40], Loss D: 0.5000000596046448, Loss G: 0.0\n"]},{"output_type":"stream","name":"stderr","text":["100%|██████████| 40/40 [00:02<00:00, 13.95it/s]\n"," 10%|█         | 4/40 [00:00<00:02, 14.94it/s]"]},{"output_type":"stream","name":"stdout","text":["Epoch [151/200], Step [1/40], Loss D: 0.5000000596046448, Loss G: 0.0\n"]},{"output_type":"stream","name":"stderr","text":["100%|██████████| 40/40 [00:02<00:00, 14.73it/s]\n","  5%|▌         | 2/40 [00:00<00:02, 13.70it/s]"]},{"output_type":"stream","name":"stdout","text":["Epoch [152/200], Step [1/40], Loss D: 0.5000000596046448, Loss G: 0.0\n"]},{"output_type":"stream","name":"stderr","text":["100%|██████████| 40/40 [00:02<00:00, 14.86it/s]\n","  5%|▌         | 2/40 [00:00<00:02, 15.25it/s]"]},{"output_type":"stream","name":"stdout","text":["Epoch [153/200], Step [1/40], Loss D: 0.5000001788139343, Loss G: 0.0\n"]},{"output_type":"stream","name":"stderr","text":["100%|██████████| 40/40 [00:02<00:00, 14.51it/s]\n","  5%|▌         | 2/40 [00:00<00:02, 14.02it/s]"]},{"output_type":"stream","name":"stdout","text":["Epoch [154/200], Step [1/40], Loss D: 0.5000000596046448, Loss G: 0.0\n"]},{"output_type":"stream","name":"stderr","text":["100%|██████████| 40/40 [00:02<00:00, 13.88it/s]\n","  5%|▌         | 2/40 [00:00<00:02, 15.35it/s]"]},{"output_type":"stream","name":"stdout","text":["Epoch [155/200], Step [1/40], Loss D: 0.5000001788139343, Loss G: 0.0\n"]},{"output_type":"stream","name":"stderr","text":["100%|██████████| 40/40 [00:02<00:00, 13.91it/s]\n","  5%|▌         | 2/40 [00:00<00:02, 13.59it/s]"]},{"output_type":"stream","name":"stdout","text":["Epoch [156/200], Step [1/40], Loss D: 0.5000002384185791, Loss G: 0.0\n"]},{"output_type":"stream","name":"stderr","text":["100%|██████████| 40/40 [00:02<00:00, 14.24it/s]\n","  5%|▌         | 2/40 [00:00<00:03, 11.48it/s]"]},{"output_type":"stream","name":"stdout","text":["Epoch [157/200], Step [1/40], Loss D: 0.5000002384185791, Loss G: 0.0\n"]},{"output_type":"stream","name":"stderr","text":["100%|██████████| 40/40 [00:02<00:00, 14.00it/s]\n","  5%|▌         | 2/40 [00:00<00:02, 13.02it/s]"]},{"output_type":"stream","name":"stdout","text":["Epoch [158/200], Step [1/40], Loss D: 0.5000001192092896, Loss G: 0.0\n"]},{"output_type":"stream","name":"stderr","text":["100%|██████████| 40/40 [00:02<00:00, 13.89it/s]\n","  5%|▌         | 2/40 [00:00<00:03, 11.65it/s]"]},{"output_type":"stream","name":"stdout","text":["Epoch [159/200], Step [1/40], Loss D: 0.5000001192092896, Loss G: 0.0\n"]},{"output_type":"stream","name":"stderr","text":["100%|██████████| 40/40 [00:03<00:00, 13.33it/s]\n","  5%|▌         | 2/40 [00:00<00:02, 14.10it/s]"]},{"output_type":"stream","name":"stdout","text":["Epoch [160/200], Step [1/40], Loss D: 0.5000004172325134, Loss G: 0.0\n"]},{"output_type":"stream","name":"stderr","text":["100%|██████████| 40/40 [00:02<00:00, 14.56it/s]\n","  5%|▌         | 2/40 [00:00<00:02, 13.09it/s]"]},{"output_type":"stream","name":"stdout","text":["Epoch [161/200], Step [1/40], Loss D: 0.5000001788139343, Loss G: 0.0\n"]},{"output_type":"stream","name":"stderr","text":["100%|██████████| 40/40 [00:02<00:00, 13.72it/s]\n","  5%|▌         | 2/40 [00:00<00:02, 13.41it/s]"]},{"output_type":"stream","name":"stdout","text":["Epoch [162/200], Step [1/40], Loss D: 0.5000001788139343, Loss G: 0.0\n"]},{"output_type":"stream","name":"stderr","text":["100%|██████████| 40/40 [00:02<00:00, 13.58it/s]\n","  5%|▌         | 2/40 [00:00<00:03, 12.58it/s]"]},{"output_type":"stream","name":"stdout","text":["Epoch [163/200], Step [1/40], Loss D: 0.5000002384185791, Loss G: 0.0\n"]},{"output_type":"stream","name":"stderr","text":["100%|██████████| 40/40 [00:02<00:00, 13.36it/s]\n","  5%|▌         | 2/40 [00:00<00:02, 13.82it/s]"]},{"output_type":"stream","name":"stdout","text":["Epoch [164/200], Step [1/40], Loss D: 0.5000001192092896, Loss G: 0.0\n"]},{"output_type":"stream","name":"stderr","text":["100%|██████████| 40/40 [00:02<00:00, 14.59it/s]\n","  5%|▌         | 2/40 [00:00<00:03, 12.55it/s]"]},{"output_type":"stream","name":"stdout","text":["Epoch [165/200], Step [1/40], Loss D: 0.5000001788139343, Loss G: 0.0\n"]},{"output_type":"stream","name":"stderr","text":["100%|██████████| 40/40 [00:02<00:00, 14.73it/s]\n","  5%|▌         | 2/40 [00:00<00:03, 11.94it/s]"]},{"output_type":"stream","name":"stdout","text":["Epoch [166/200], Step [1/40], Loss D: 0.5000000596046448, Loss G: 0.0\n"]},{"output_type":"stream","name":"stderr","text":["100%|██████████| 40/40 [00:02<00:00, 14.39it/s]\n"," 10%|█         | 4/40 [00:00<00:02, 14.41it/s]"]},{"output_type":"stream","name":"stdout","text":["Epoch [167/200], Step [1/40], Loss D: 0.5000000596046448, Loss G: 0.0\n"]},{"output_type":"stream","name":"stderr","text":["100%|██████████| 40/40 [00:02<00:00, 13.90it/s]\n","  5%|▌         | 2/40 [00:00<00:02, 13.22it/s]"]},{"output_type":"stream","name":"stdout","text":["Epoch [168/200], Step [1/40], Loss D: 0.5000000596046448, Loss G: 0.0\n"]},{"output_type":"stream","name":"stderr","text":["100%|██████████| 40/40 [00:02<00:00, 14.10it/s]\n","  5%|▌         | 2/40 [00:00<00:02, 14.95it/s]"]},{"output_type":"stream","name":"stdout","text":["Epoch [169/200], Step [1/40], Loss D: 0.5000000596046448, Loss G: 0.0\n"]},{"output_type":"stream","name":"stderr","text":["100%|██████████| 40/40 [00:02<00:00, 14.55it/s]\n","  5%|▌         | 2/40 [00:00<00:02, 13.86it/s]"]},{"output_type":"stream","name":"stdout","text":["Epoch [170/200], Step [1/40], Loss D: 0.5000004768371582, Loss G: 0.0\n"]},{"output_type":"stream","name":"stderr","text":["100%|██████████| 40/40 [00:02<00:00, 13.96it/s]\n","  5%|▌         | 2/40 [00:00<00:02, 14.05it/s]"]},{"output_type":"stream","name":"stdout","text":["Epoch [171/200], Step [1/40], Loss D: 0.5000003576278687, Loss G: 0.0\n"]},{"output_type":"stream","name":"stderr","text":["100%|██████████| 40/40 [00:02<00:00, 13.72it/s]\n","  5%|▌         | 2/40 [00:00<00:03, 11.68it/s]"]},{"output_type":"stream","name":"stdout","text":["Epoch [172/200], Step [1/40], Loss D: 0.5000001192092896, Loss G: 0.0\n"]},{"output_type":"stream","name":"stderr","text":["100%|██████████| 40/40 [00:02<00:00, 13.57it/s]\n","  5%|▌         | 2/40 [00:00<00:03, 11.65it/s]"]},{"output_type":"stream","name":"stdout","text":["Epoch [173/200], Step [1/40], Loss D: 0.5000000596046448, Loss G: 0.0\n"]},{"output_type":"stream","name":"stderr","text":["100%|██████████| 40/40 [00:02<00:00, 14.64it/s]\n","  5%|▌         | 2/40 [00:00<00:03, 12.46it/s]"]},{"output_type":"stream","name":"stdout","text":["Epoch [174/200], Step [1/40], Loss D: 0.5000000596046448, Loss G: 0.0\n"]},{"output_type":"stream","name":"stderr","text":["100%|██████████| 40/40 [00:02<00:00, 14.32it/s]\n","  5%|▌         | 2/40 [00:00<00:02, 13.23it/s]"]},{"output_type":"stream","name":"stdout","text":["Epoch [175/200], Step [1/40], Loss D: 0.5000001192092896, Loss G: 0.0\n"]},{"output_type":"stream","name":"stderr","text":["100%|██████████| 40/40 [00:02<00:00, 14.32it/s]\n","  5%|▌         | 2/40 [00:00<00:02, 13.56it/s]"]},{"output_type":"stream","name":"stdout","text":["Epoch [176/200], Step [1/40], Loss D: 0.5000000596046448, Loss G: 0.0\n"]},{"output_type":"stream","name":"stderr","text":["100%|██████████| 40/40 [00:03<00:00, 13.25it/s]\n","  5%|▌         | 2/40 [00:00<00:02, 14.29it/s]"]},{"output_type":"stream","name":"stdout","text":["Epoch [177/200], Step [1/40], Loss D: 0.5000000596046448, Loss G: 0.0\n"]},{"output_type":"stream","name":"stderr","text":["100%|██████████| 40/40 [00:02<00:00, 14.71it/s]\n","  5%|▌         | 2/40 [00:00<00:02, 13.96it/s]"]},{"output_type":"stream","name":"stdout","text":["Epoch [178/200], Step [1/40], Loss D: 0.5000001788139343, Loss G: 0.0\n"]},{"output_type":"stream","name":"stderr","text":["100%|██████████| 40/40 [00:02<00:00, 14.22it/s]\n","  5%|▌         | 2/40 [00:00<00:02, 14.02it/s]"]},{"output_type":"stream","name":"stdout","text":["Epoch [179/200], Step [1/40], Loss D: 0.5000001192092896, Loss G: 0.0\n"]},{"output_type":"stream","name":"stderr","text":["100%|██████████| 40/40 [00:02<00:00, 14.35it/s]\n","  5%|▌         | 2/40 [00:00<00:02, 15.68it/s]"]},{"output_type":"stream","name":"stdout","text":["Epoch [180/200], Step [1/40], Loss D: 0.5000000596046448, Loss G: 0.0\n"]},{"output_type":"stream","name":"stderr","text":["100%|██████████| 40/40 [00:02<00:00, 14.24it/s]\n","  5%|▌         | 2/40 [00:00<00:03, 12.13it/s]"]},{"output_type":"stream","name":"stdout","text":["Epoch [181/200], Step [1/40], Loss D: 0.5000001788139343, Loss G: 0.0\n"]},{"output_type":"stream","name":"stderr","text":["100%|██████████| 40/40 [00:02<00:00, 14.35it/s]\n"," 10%|█         | 4/40 [00:00<00:02, 14.47it/s]"]},{"output_type":"stream","name":"stdout","text":["Epoch [182/200], Step [1/40], Loss D: 0.5000001192092896, Loss G: 0.0\n"]},{"output_type":"stream","name":"stderr","text":["100%|██████████| 40/40 [00:02<00:00, 14.86it/s]\n"," 10%|█         | 4/40 [00:00<00:02, 14.18it/s]"]},{"output_type":"stream","name":"stdout","text":["Epoch [183/200], Step [1/40], Loss D: 0.5000000596046448, Loss G: 0.0\n"]},{"output_type":"stream","name":"stderr","text":["100%|██████████| 40/40 [00:02<00:00, 14.31it/s]\n","  5%|▌         | 2/40 [00:00<00:02, 13.56it/s]"]},{"output_type":"stream","name":"stdout","text":["Epoch [184/200], Step [1/40], Loss D: 0.5000000596046448, Loss G: 0.0\n"]},{"output_type":"stream","name":"stderr","text":["100%|██████████| 40/40 [00:03<00:00, 12.87it/s]\n","  5%|▌         | 2/40 [00:00<00:03, 11.79it/s]"]},{"output_type":"stream","name":"stdout","text":["Epoch [185/200], Step [1/40], Loss D: 0.5, Loss G: 0.0\n"]},{"output_type":"stream","name":"stderr","text":["100%|██████████| 40/40 [00:03<00:00, 13.03it/s]\n","  5%|▌         | 2/40 [00:00<00:02, 13.32it/s]"]},{"output_type":"stream","name":"stdout","text":["Epoch [186/200], Step [1/40], Loss D: 0.5000000596046448, Loss G: 0.0\n"]},{"output_type":"stream","name":"stderr","text":["100%|██████████| 40/40 [00:02<00:00, 14.10it/s]\n","  5%|▌         | 2/40 [00:00<00:02, 14.32it/s]"]},{"output_type":"stream","name":"stdout","text":["Epoch [187/200], Step [1/40], Loss D: 0.5000000596046448, Loss G: 0.0\n"]},{"output_type":"stream","name":"stderr","text":["100%|██████████| 40/40 [00:02<00:00, 14.18it/s]\n","  5%|▌         | 2/40 [00:00<00:02, 13.97it/s]"]},{"output_type":"stream","name":"stdout","text":["Epoch [188/200], Step [1/40], Loss D: 0.5000001788139343, Loss G: 0.0\n"]},{"output_type":"stream","name":"stderr","text":["100%|██████████| 40/40 [00:02<00:00, 13.95it/s]\n","  5%|▌         | 2/40 [00:00<00:03, 12.07it/s]"]},{"output_type":"stream","name":"stdout","text":["Epoch [189/200], Step [1/40], Loss D: 0.5000000596046448, Loss G: 0.0\n"]},{"output_type":"stream","name":"stderr","text":["100%|██████████| 40/40 [00:03<00:00, 13.27it/s]\n","  5%|▌         | 2/40 [00:00<00:03, 11.34it/s]"]},{"output_type":"stream","name":"stdout","text":["Epoch [190/200], Step [1/40], Loss D: 0.5000000596046448, Loss G: 0.0\n"]},{"output_type":"stream","name":"stderr","text":["100%|██████████| 40/40 [00:02<00:00, 14.25it/s]\n","  5%|▌         | 2/40 [00:00<00:02, 12.85it/s]"]},{"output_type":"stream","name":"stdout","text":["Epoch [191/200], Step [1/40], Loss D: 0.5000000596046448, Loss G: 0.0\n"]},{"output_type":"stream","name":"stderr","text":["100%|██████████| 40/40 [00:02<00:00, 14.15it/s]\n","  5%|▌         | 2/40 [00:00<00:03, 12.62it/s]"]},{"output_type":"stream","name":"stdout","text":["Epoch [192/200], Step [1/40], Loss D: 0.5000000596046448, Loss G: 0.0\n"]},{"output_type":"stream","name":"stderr","text":["100%|██████████| 40/40 [00:02<00:00, 13.88it/s]\n","  5%|▌         | 2/40 [00:00<00:02, 12.86it/s]"]},{"output_type":"stream","name":"stdout","text":["Epoch [193/200], Step [1/40], Loss D: 0.5000000596046448, Loss G: 0.0\n"]},{"output_type":"stream","name":"stderr","text":["100%|██████████| 40/40 [00:03<00:00, 13.21it/s]\n","  5%|▌         | 2/40 [00:00<00:02, 13.69it/s]"]},{"output_type":"stream","name":"stdout","text":["Epoch [194/200], Step [1/40], Loss D: 0.5000001192092896, Loss G: 0.0\n"]},{"output_type":"stream","name":"stderr","text":["100%|██████████| 40/40 [00:02<00:00, 14.06it/s]\n","  5%|▌         | 2/40 [00:00<00:02, 13.40it/s]"]},{"output_type":"stream","name":"stdout","text":["Epoch [195/200], Step [1/40], Loss D: 0.5000001192092896, Loss G: 0.0\n"]},{"output_type":"stream","name":"stderr","text":["100%|██████████| 40/40 [00:02<00:00, 14.32it/s]\n","  5%|▌         | 2/40 [00:00<00:02, 14.36it/s]"]},{"output_type":"stream","name":"stdout","text":["Epoch [196/200], Step [1/40], Loss D: 0.5000000596046448, Loss G: 0.0\n"]},{"output_type":"stream","name":"stderr","text":["100%|██████████| 40/40 [00:02<00:00, 14.62it/s]\n"," 10%|█         | 4/40 [00:00<00:02, 14.65it/s]"]},{"output_type":"stream","name":"stdout","text":["Epoch [197/200], Step [1/40], Loss D: 0.5000000596046448, Loss G: 0.0\n"]},{"output_type":"stream","name":"stderr","text":["100%|██████████| 40/40 [00:02<00:00, 14.23it/s]\n","  5%|▌         | 2/40 [00:00<00:03, 12.66it/s]"]},{"output_type":"stream","name":"stdout","text":["Epoch [198/200], Step [1/40], Loss D: 0.5, Loss G: 0.0\n"]},{"output_type":"stream","name":"stderr","text":["100%|██████████| 40/40 [00:02<00:00, 13.36it/s]\n","  5%|▌         | 2/40 [00:00<00:03, 11.81it/s]"]},{"output_type":"stream","name":"stdout","text":["Epoch [199/200], Step [1/40], Loss D: 0.5, Loss G: 0.0\n"]},{"output_type":"stream","name":"stderr","text":["100%|██████████| 40/40 [00:02<00:00, 13.73it/s]\n","  5%|▌         | 2/40 [00:00<00:03, 11.42it/s]"]},{"output_type":"stream","name":"stdout","text":["Epoch [200/200], Step [1/40], Loss D: 0.5000001192092896, Loss G: 0.0\n"]},{"output_type":"stream","name":"stderr","text":["100%|██████████| 40/40 [00:02<00:00, 13.86it/s]\n"]}]},{"cell_type":"code","source":["import os\n","import torch\n","import torch.nn as nn\n","import torch.optim as optim\n","from torch.utils.data import DataLoader, Dataset\n","from torchvision import transforms\n","from PIL import Image\n","import matplotlib.pyplot as plt\n","import torchvision\n","\n","# CUDA 사용 여부\n","device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n","\n","# 유넷 스타일 Generator 정의 (출력 크기를 128x128로 설정)\n","class UNetGenerator(nn.Module):\n","    def __init__(self):\n","        super(UNetGenerator, self).__init__()\n","        self.encoder = nn.Sequential(\n","            nn.Conv2d(1, 64, kernel_size=4, stride=2, padding=1),\n","            nn.ReLU(inplace=True),\n","            nn.Conv2d(64, 128, kernel_size=4, stride=2, padding=1),\n","            nn.ReLU(inplace=True),\n","            nn.Conv2d(128, 256, kernel_size=4, stride=2, padding=1),\n","            nn.ReLU(inplace=True),\n","        )\n","\n","        self.bottleneck = nn.Conv2d(256, 512, kernel_size=4, stride=2, padding=1)\n","\n","        self.decoder = nn.Sequential(\n","            nn.ConvTranspose2d(512, 256, kernel_size=4, stride=2, padding=1),\n","            nn.ReLU(inplace=True),\n","            nn.ConvTranspose2d(256, 128, kernel_size=4, stride=2, padding=1),\n","            nn.ReLU(inplace=True),\n","            nn.ConvTranspose2d(128, 64, kernel_size=4, stride=2, padding=1),\n","            nn.ReLU(inplace=True),\n","        )\n","\n","        self.output_layer = nn.Conv2d(64, 1, kernel_size=3, stride=1, padding=1)\n","\n","    def forward(self, x):\n","        x1 = self.encoder(x)\n","        x2 = self.bottleneck(x1)\n","        x3 = self.decoder(x2)\n","        return self.output_layer(x3)\n","\n","# Discriminator 정의 (입력 크기를 128x128로 설정)\n","class Discriminator(nn.Module):\n","    def __init__(self):\n","        super(Discriminator, self).__init__()\n","        self.model = nn.Sequential(\n","            nn.Conv2d(1, 64, kernel_size=4, stride=2, padding=1),\n","            nn.LeakyReLU(0.2, inplace=True),\n","            nn.Conv2d(64, 128, kernel_size=4, stride=2, padding=1),\n","            nn.LeakyReLU(0.2, inplace=True),\n","            nn.Conv2d(128, 256, kernel_size=4, stride=2, padding=1),\n","            nn.LeakyReLU(0.2, inplace=True),\n","            nn.Conv2d(256, 512, kernel_size=4, stride=2, padding=1),\n","            nn.LeakyReLU(0.2, inplace=True),\n","            nn.Conv2d(512, 1, kernel_size=4, stride=2, padding=1),  # Output size will be 1x16x16\n","            nn.Sigmoid()\n","        )\n","\n","    def forward(self, x):\n","        return self.model(x)\n","\n","# 손실 함수 및 최적화 함수\n","adversarial_loss = nn.BCELoss()\n","generator = UNetGenerator().to(device)\n","discriminator = Discriminator().to(device)\n","\n","optimizer_G = optim.Adam(generator.parameters(), lr=0.0002, betas=(0.5, 0.999))\n","optimizer_D = optim.Adam(discriminator.parameters(), lr=0.0002, betas=(0.5, 0.999))\n","\n","# 데이터셋 클래스 (단일 폴더에 이미지들이 있는 경우)\n","class ImageDataset(Dataset):\n","    def __init__(self, image_folder, transform=None):\n","        self.image_folder = image_folder\n","        self.transform = transform\n","        self.image_files = [f for f in os.listdir(image_folder) if f.endswith(('png', 'jpg', 'jpeg'))]\n","\n","    def __len__(self):\n","        return len(self.image_files)\n","\n","    def __getitem__(self, idx):\n","        img_path = os.path.join(self.image_folder, self.image_files[idx])\n","        img = Image.open(img_path).convert('L')  # 그레이스케일로 변환\n","        if self.transform:\n","            img = self.transform(img)\n","        return img\n","\n","# 데이터 전처리 및 DataLoader 설정\n","transform = transforms.Compose([\n","    transforms.Resize((128, 128)),  # Generator와 Discriminator의 크기를 128x128로 설정\n","    transforms.ToTensor(),\n","    transforms.Normalize((0.5,), (0.5,))\n","])\n","\n","# 이미지 폴더 경로\n","image_folder_path = '/content/drive/MyDrive/GANFACE/filtered_images'\n","train_dataset = ImageDataset(image_folder_path, transform)\n","train_dataloader = DataLoader(train_dataset, batch_size=16, shuffle=True)\n","\n","# 학습 함수\n","def train(generator, discriminator, dataloader, num_epochs=100, device=device):\n","    for epoch in range(num_epochs):\n","        for i, imgs in enumerate(dataloader):\n","            imgs = imgs.to(device)\n","\n","            # 진짜 이미지에 대한 라벨 생성\n","            valid = torch.ones(imgs.size(0), 1, 16, 16).to(device)  # True는 1\n","            fake = torch.zeros(imgs.size(0), 1, 16, 16).to(device)  # Fake는 0\n","\n","            # Generator 훈련\n","            optimizer_G.zero_grad()\n","\n","            gen_imgs = generator(imgs)\n","            g_loss = adversarial_loss(discriminator(gen_imgs), valid)\n","\n","            g_loss.backward()\n","            optimizer_G.step()\n","\n","            # Discriminator 훈련\n","            optimizer_D.zero_grad()\n","\n","            # 진짜 이미지에 대한 손실\n","            real_loss = adversarial_loss(discriminator(imgs), valid)\n","            # 가짜 이미지에 대한 손실\n","            fake_loss = adversarial_loss(discriminator(gen_imgs.detach()), fake)\n","            d_loss = (real_loss + fake_loss) / 2\n","\n","            d_loss.backward()\n","            optimizer_D.step()\n","\n","            # 로그 출력\n","            if i % 20 == 0:\n","                print(f\"Epoch [{epoch+1}/{num_epochs}], Step [{i+1}/{len(dataloader)}], \"\n","                      f\"Loss D: {d_loss.item()}, Loss G: {g_loss.item()}\")\n","\n","        # 생성된 이미지 저장\n","        if (epoch + 1) % 10 == 0:\n","            save_fake_images(epoch, gen_imgs)\n","\n","# 생성된 이미지 저장 및 출력\n","def save_fake_images(epoch, fake_images):\n","    fake_images = fake_images.detach().cpu()\n","    grid = torchvision.utils.make_grid(fake_images, normalize=True)\n","    plt.figure(figsize=(8,8))\n","    plt.imshow(grid.permute(1, 2, 0))\n","    plt.axis('off')\n","\n","    # 이미지 저장\n","    save_path = '/content/drive/MyDrive/GANFACE/output'\n","    os.makedirs(save_path, exist_ok=True)\n","    plt.savefig(os.path.join(save_path, f\"fake_images_epoch_{epoch+1}.png\"))\n","    plt.close()\n","\n","# 모델 훈련\n","train(generator, discriminator, train_dataloader, num_epochs=200, device=device)\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":356},"id":"13MGGq6yjq6u","executionInfo":{"status":"error","timestamp":1733901449746,"user_tz":-540,"elapsed":2183,"user":{"displayName":"현우","userId":"14235240467528686656"}},"outputId":"d29ad2be-6798-4271-e96c-386c79c5f03e"},"execution_count":19,"outputs":[{"output_type":"error","ename":"ValueError","evalue":"Using a target size (torch.Size([16, 1, 16, 16])) that is different to the input size (torch.Size([16, 1, 2, 2])) is deprecated. Please ensure they have the same size.","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)","\u001b[0;32m<ipython-input-19-6d6643efb533>\u001b[0m in \u001b[0;36m<cell line: 158>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    156\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    157\u001b[0m \u001b[0;31m# 모델 훈련\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 158\u001b[0;31m \u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgenerator\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdiscriminator\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_dataloader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_epochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m200\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m","\u001b[0;32m<ipython-input-19-6d6643efb533>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(generator, discriminator, dataloader, num_epochs, device)\u001b[0m\n\u001b[1;32m    115\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    116\u001b[0m             \u001b[0mgen_imgs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgenerator\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimgs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 117\u001b[0;31m             \u001b[0mg_loss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0madversarial_loss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdiscriminator\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgen_imgs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalid\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    118\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    119\u001b[0m             \u001b[0mg_loss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1734\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1735\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1736\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1737\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1738\u001b[0m     \u001b[0;31m# torchrec tests the code consistency with the following code\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1745\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1746\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1747\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1748\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1749\u001b[0m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/loss.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input, target)\u001b[0m\n\u001b[1;32m    695\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    696\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 697\u001b[0;31m         return F.binary_cross_entropy(\n\u001b[0m\u001b[1;32m    698\u001b[0m             \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweight\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreduction\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreduction\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    699\u001b[0m         )\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/functional.py\u001b[0m in \u001b[0;36mbinary_cross_entropy\u001b[0;34m(input, target, weight, size_average, reduce, reduction)\u001b[0m\n\u001b[1;32m   3543\u001b[0m         \u001b[0mreduction_enum\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_Reduction\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_enum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mreduction\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3544\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mtarget\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3545\u001b[0;31m         raise ValueError(\n\u001b[0m\u001b[1;32m   3546\u001b[0m             \u001b[0;34mf\"Using a target size ({target.size()}) that is different to the input size ({input.size()}) is deprecated. \"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3547\u001b[0m             \u001b[0;34m\"Please ensure they have the same size.\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mValueError\u001b[0m: Using a target size (torch.Size([16, 1, 16, 16])) that is different to the input size (torch.Size([16, 1, 2, 2])) is deprecated. Please ensure they have the same size."]}]},{"cell_type":"markdown","metadata":{"id":"_s8Qp4iQgqme"},"source":["알겠습니다! 전체 훈련 코드와 함께 **스케치 이미지**(`image.png`)를 사용하고, 훈련 후 **모델 저장** 및 **성능 평가**를 위한 코드까지 포함하겠습니다.\n","\n","아래는 Google Colab에서 실행할 수 있는 **전체 코드**입니다.\n","\n","### 1. **코드 설명**\n","- **데이터 로딩 및 전처리**: 스케치 이미지를 `GANFACE` 폴더에서 불러와서 훈련에 사용합니다.\n","- **모델 훈련**: CycleGAN을 사용하여 **스케치 -> 얼굴 이미지** 변환을 위한 훈련을 진행합니다.\n","- **모델 저장**: 훈련 후 모델을 Google Drive에 저장합니다.\n","- **성능 평가**: 훈련 중 생성된 이미지를 저장하고, 성능을 평가합니다.\n","\n","### 2. **전체 코드**\n","\n","```python\n","# Google Drive 마운트\n","from google.colab import drive\n","drive.mount('/content/drive')\n","\n","# 필요한 라이브러리 설치\n","!pip install torch torchvision matplotlib numpy pillow\n","\n","import os\n","import torch\n","import torch.nn as nn\n","import torch.nn.functional as F\n","from torchvision import datasets, transforms\n","from torch.utils.data import DataLoader\n","from torchvision.utils import save_image\n","import matplotlib.pyplot as plt\n","\n","# Google Drive에서 이미지 경로 설정\n","image_folder_path = '/content/drive/MyDrive/GANFACE/filtered_images'\n","\n","# 이미지 전처리 설정 (Resize, Normalize, ToTensor 등)\n","transform = transforms.Compose([\n","    transforms.Resize(256),  # 256x256 크기로 리사이즈\n","    transforms.ToTensor(),   # 텐서로 변환\n","    transforms.Normalize((0.5,), (0.5,)),  # 정규화 (0.5, 0.5)로 예시\n","])\n","\n","# ImageFolder를 사용하여 데이터셋 로드\n","train_dataset = datasets.ImageFolder(root=image_folder_path, transform=transform)\n","\n","# DataLoader 설정 (배치 사이즈 16)\n","train_dataloader = DataLoader(train_dataset, batch_size=16, shuffle=True)\n","\n","# Self-Attention Layer\n","class SelfAttention(nn.Module):\n","    def __init__(self, in_channels):\n","        super(SelfAttention, self).__init__()\n","        \n","        self.query_conv = nn.Conv2d(in_channels, in_channels // 8, kernel_size=1)\n","        self.key_conv = nn.Conv2d(in_channels, in_channels // 8, kernel_size=1)\n","        self.value_conv = nn.Conv2d(in_channels, in_channels, kernel_size=1)\n","        self.gamma = nn.Parameter(torch.zeros(1))\n","\n","    def forward(self, x):\n","        batch_size, channels, height, width = x.size()\n","\n","        query = self.query_conv(x).view(batch_size, -1, height * width)\n","        key = self.key_conv(x).view(batch_size, -1, height * width)\n","        value = self.value_conv(x).view(batch_size, -1, height * width)\n","\n","        attention = torch.bmm(query.permute(0, 2, 1), key)\n","        attention = F.softmax(attention, dim=-1)\n","\n","        out = torch.bmm(value, attention.permute(0, 2, 1))\n","        out = out.view(batch_size, channels, height, width)\n","\n","        return self.gamma * out + x\n","\n","# Spatial Attention Layer\n","class SpatialAttention(nn.Module):\n","    def __init__(self, in_channels):\n","        super(SpatialAttention, self).__init__()\n","        self.conv1 = nn.Conv2d(in_channels, 1, kernel_size=7, stride=1, padding=3)\n","\n","    def forward(self, x):\n","        attention_map = torch.sigmoid(self.conv1(x))\n","        return x * attention_map\n","\n","# Generator 모델 정의\n","class GeneratorWithAttention(nn.Module):\n","    def __init__(self, input_channels, output_channels):\n","        super(GeneratorWithAttention, self).__init__()\n","\n","        # Encoder\n","        self.encoder = nn.Sequential(\n","            nn.Conv2d(input_channels, 64, kernel_size=4, stride=2, padding=1),\n","            nn.ReLU(),\n","            nn.Conv2d(64, 128, kernel_size=4, stride=2, padding=1),\n","            nn.ReLU(),\n","            nn.Conv2d(128, 256, kernel_size=4, stride=2, padding=1),\n","            nn.ReLU(),\n","            SelfAttention(256),\n","            nn.Conv2d(256, 512, kernel_size=4, stride=2, padding=1),\n","            nn.ReLU(),\n","            SpatialAttention(512),\n","        )\n","\n","        # Decoder\n","        self.decoder = nn.Sequential(\n","            nn.ConvTranspose2d(512, 256, kernel_size=4, stride=2, padding=1),\n","            nn.ReLU(),\n","            nn.ConvTranspose2d(256, 128, kernel_size=4, stride=2, padding=1),\n","            nn.ReLU(),\n","            nn.ConvTranspose2d(128, 64, kernel_size=4, stride=2, padding=1),\n","            nn.ReLU(),\n","            nn.ConvTranspose2d(64, output_channels, kernel_size=4, stride=2, padding=1),\n","            nn.Tanh(),\n","        )\n","\n","    def forward(self, x):\n","        x = self.encoder(x)\n","        x = self.decoder(x)\n","        return x\n","\n","# Discriminator 모델 정의\n","class Discriminator(nn.Module):\n","    def __init__(self, input_channels):\n","        super(Discriminator, self).__init__()\n","\n","        self.model = nn.Sequential(\n","            nn.Conv2d(input_channels, 64, kernel_size=4, stride=2, padding=1),\n","            nn.LeakyReLU(0.2),\n","            nn.Conv2d(64, 128, kernel_size=4, stride=2, padding=1),\n","            nn.LeakyReLU(0.2),\n","            nn.Conv2d(128, 256, kernel_size=4, stride=2, padding=1),\n","            nn.LeakyReLU(0.2),\n","            nn.Conv2d(256, 512, kernel_size=4, stride=2, padding=1),\n","            nn.LeakyReLU(0.2),\n","            nn.Conv2d(512, 1, kernel_size=4, stride=1, padding=1),\n","            nn.Sigmoid(),\n","        )\n","\n","    def forward(self, x):\n","        return self.model(x)\n","\n","# CycleGAN 모델 정의\n","class CycleGAN(nn.Module):\n","    def __init__(self, input_channels, output_channels):\n","        super(CycleGAN, self).__init__()\n","        self.generator_AtoB = GeneratorWithAttention(input_channels, output_channels)\n","        self.generator_BtoA = GeneratorWithAttention(output_channels, input_channels)\n","        self.discriminator_A = Discriminator(input_channels)\n","        self.discriminator_B = Discriminator(output_channels)\n","\n","    def forward(self, x):\n","        pass  # forward()는 실제로 필요없음\n","\n","# Loss 함수\n","def adversarial_loss(disc_pred, target):\n","    return torch.mean((disc_pred - target) ** 2)\n","\n","def cycle_consistency_loss(real_image, reconstructed_image):\n","    return torch.mean((real_image - reconstructed_image) ** 2)\n","\n","# 훈련 루프\n","def train(cyclegan, dataloader, num_epochs, device):\n","    optimizer_g = torch.optim.Adam(cyclegan.parameters(), lr=0.0002, betas=(0.5, 0.999))\n","    optimizer_d = torch.optim.Adam(cyclegan.parameters(), lr=0.0002, betas=(0.5, 0.999))\n","\n","    for epoch in range(num_epochs):\n","        for i, data in enumerate(dataloader):\n","            real_A, real_B = data\n","            real_A, real_B = real_A.to(device), real_B.to(device)\n","\n","            # Generator A->B 업데이트 (스케치 -> 얼굴 사진)\n","            optimizer_g.zero_grad()\n","            fake_B = cyclegan.generator_AtoB(real_A)\n","            loss_G_AtoB = adversarial_loss(cyclegan.discriminator_B(fake_B), torch.ones_like(fake_B)) \\\n","                          + cycle_consistency_loss(real_A, cyclegan.generator_BtoA(fake_B))\n","            loss_G_AtoB.backward()\n","            optimizer_g.step()\n","\n","            # Discriminator A 업데이트\n","            optimizer_d.zero_grad()\n","            loss_D_A = adversarial_loss(cyclegan.discriminator_A(real_A), torch.ones_like(real_A)) \\\n","                       + adversarial_loss(cyclegan.discriminator_A(fake_B.detach()), torch.zeros_like(fake_B))\n","            loss_D_A.backward()\n","            optimizer_d.step()\n","\n","            # Discriminator B 업데이트\n","            optimizer_d.zero_grad()\n","            loss_D_B = adversarial_loss(cyclegan.discriminator_B(real_B), torch.ones_like(real_B)) \\\n","                       + adversarial_loss(cyclegan.discriminator_B(fake_B.detach()), torch.zeros_like(fake_B))\n","            loss_D_B.backward()\n","            optimizer_d.step()\n","\n","            if i % 100 == 0:\n","                print(f\"[Epoch {epoch}/{num_epochs}] [Batch {i}/{len(dataloader)}] \"\n","                      f\"[D loss: {loss_D_A.item() + loss_D_B.item()}] \"\n","                      f\"[G loss: {loss_G_AtoB.item()}]\")\n","\n","        # 중간 결과 저장\n","        if epoch % 10 == 0:\n","            save_image(fake_B.data[:25], f\"/content/drive/MyDrive/GANFACE/generated_images_epoch_{epoch}.png\", nrow=5, normalize=True)\n","\n","    # 훈련 완료 후 모델 저장\n","    torch.save(cyclegan.state_dict(), '/content/drive/MyDrive/GANFACE/cyclegan_model.pth')\n","    print(\"Model saved successfully!\")\n","\n","# GPU 설정\n","device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n","\n","# CycleGAN 모델 인스턴스화\n","cyclegan = CycleGAN(input_channels=3, output_channels=3).to(device)\n","\n","# 훈련 시작\n","train(cyclegan, train_dataloader, num_epochs=200, device=device)\n","```\n","\n","### 3. **주요 작업 설명**\n","\n","- **Google Drive에 파일을 저장**: 모델은 훈련이 끝난 후 Google Drive에 저장됩니다. `cyclegan_model.pth`로 저장됩니다.\n","- **성능 평가**: 훈련 중 중간 결과를 Google Drive에 이미지로 저장합니다. 예를 들어, `generated_images_epoch_{epoch}.png` 파일에 저장됩니다.\n","- **훈련**: 스케치 이미지를 변환하기 위해 **스케치 -> 얼굴 이미지** 변환을 위한 **Generator A->B**를 훈련합니다.\n","- **훈련 후 모델 저장**: 훈련이 끝나면 모델을 Google Drive에 저장합니다.\n","\n","### 4. **성능 평가**\n","- 훈련 중마다 생성된 이미지를 저장하여 모델의 성능을 시각적으로 평가할 수 있습니다.\n","- 훈련이 완료된 후, **`generated_images_epoch_{epoch}.png`** 파일을 확인하고, **모델 성능**을 표로 정리하는 방법은 이미지들을 통해 판단할 수 있습니다.\n","\n","### 5. **결과 확인**\n","훈련이 완료된 후, `GANFACE` 폴더에 `generated_images_epoch_{epoch}.png`와 `cyclegan_model.pth` 파일이 생성됩니다. 훈련 중에 모델 성능을 평가하고, 최종적으로 저장된 모델을 활용하여 **스케치 이미지를 얼굴 이미지로 변환**할 수 있습니다.\n","\n","---\n","\n","위 코드를 Colab에서 실행하여 훈련하고 결과를 저장할 수 있습니다."]},{"cell_type":"markdown","metadata":{"id":"2WyhoUy8fW2I"},"source":["Google Colab에서 스케치 이미지를 실제 얼굴 사진으로 변환하는 CycleGAN 모델을 훈련하는 전체 코드를 제공하겠습니다. 여기에는 Google Drive의 데이터를 불러오고, CycleGAN을 학습시키는 과정이 포함됩니다.\n","\n","### Google Colab에서 전체 코드\n","\n","#### 1. Google Drive 마운트 및 라이브러리 설치\n","\n","```python\n","# Google Drive 마운트\n","from google.colab import drive\n","drive.mount('/content/drive')\n","```\n","\n","Google Drive에 있는 **`GANFACE/filtered_images`** 폴더에서 스케치 및 얼굴 이미지를 불러옵니다.\n","\n","#### 2. 필요한 라이브러리 설치\n","\n","```python\n","!pip install torch torchvision matplotlib numpy pillow\n","```\n","\n","#### 3. 데이터셋 준비\n","\n","이제 **Google Drive**에 있는 이미지 데이터를 로드하고 훈련 데이터로 준비합니다.\n","\n","```python\n","import os\n","from torchvision import datasets, transforms\n","from torch.utils.data import DataLoader\n","\n","# Google Drive에서 이미지 경로 설정\n","image_folder_path = '/content/drive/MyDrive/GANFACE/filtered_images'\n","\n","# 이미지 전처리 설정 (Resize, Normalize, ToTensor 등)\n","transform = transforms.Compose([\n","    transforms.Resize(256),  # 256x256 크기로 리사이즈\n","    transforms.ToTensor(),   # 텐서로 변환\n","    transforms.Normalize((0.5,), (0.5,)),  # 정규화 (0.5, 0.5)로 예시\n","])\n","\n","# ImageFolder를 사용하여 데이터셋 로드\n","train_dataset = datasets.ImageFolder(root=image_folder_path, transform=transform)\n","\n","# DataLoader 설정 (배치 사이즈 16)\n","train_dataloader = DataLoader(train_dataset, batch_size=16, shuffle=True)\n","```\n","\n","#### 4. Attention을 포함한 CycleGAN 모델 구현\n","\n","CycleGAN 모델을 정의합니다. 이 모델은 스케치 이미지를 실제 얼굴 이미지로 변환하는 역할을 합니다.\n","\n","##### 4.1. Attention Layer (Self-Attention과 Spatial Attention)\n","\n","```python\n","import torch\n","import torch.nn as nn\n","import torch.nn.functional as F\n","\n","# Self-Attention Layer\n","class SelfAttention(nn.Module):\n","    def __init__(self, in_channels):\n","        super(SelfAttention, self).__init__()\n","        \n","        self.query_conv = nn.Conv2d(in_channels, in_channels // 8, kernel_size=1)\n","        self.key_conv = nn.Conv2d(in_channels, in_channels // 8, kernel_size=1)\n","        self.value_conv = nn.Conv2d(in_channels, in_channels, kernel_size=1)\n","        self.gamma = nn.Parameter(torch.zeros(1))\n","\n","    def forward(self, x):\n","        batch_size, channels, height, width = x.size()\n","\n","        # Compute query, key, value\n","        query = self.query_conv(x).view(batch_size, -1, height * width)\n","        key = self.key_conv(x).view(batch_size, -1, height * width)\n","        value = self.value_conv(x).view(batch_size, -1, height * width)\n","\n","        # Scaled dot-product attention\n","        attention = torch.bmm(query.permute(0, 2, 1), key)  # Q*K^T\n","        attention = F.softmax(attention, dim=-1)\n","\n","        out = torch.bmm(value, attention.permute(0, 2, 1))  # V*Attention\n","        out = out.view(batch_size, channels, height, width)\n","\n","        return self.gamma * out + x  # Apply attention and residual connection\n","\n","# Spatial Attention Layer\n","class SpatialAttention(nn.Module):\n","    def __init__(self, in_channels):\n","        super(SpatialAttention, self).__init__()\n","        self.conv1 = nn.Conv2d(in_channels, 1, kernel_size=7, stride=1, padding=3)\n","\n","    def forward(self, x):\n","        attention_map = torch.sigmoid(self.conv1(x))  # Apply sigmoid to create attention map\n","        return x * attention_map\n","```\n","\n","##### 4.2. Generator 및 Discriminator 모델 정의\n","\n","```python\n","# Generator 모델 정의\n","class GeneratorWithAttention(nn.Module):\n","    def __init__(self, input_channels, output_channels):\n","        super(GeneratorWithAttention, self).__init__()\n","\n","        # Encoder\n","        self.encoder = nn.Sequential(\n","            nn.Conv2d(input_channels, 64, kernel_size=4, stride=2, padding=1),\n","            nn.ReLU(),\n","            nn.Conv2d(64, 128, kernel_size=4, stride=2, padding=1),\n","            nn.ReLU(),\n","            nn.Conv2d(128, 256, kernel_size=4, stride=2, padding=1),\n","            nn.ReLU(),\n","            SelfAttention(256),  # Self-Attention 추가\n","            nn.Conv2d(256, 512, kernel_size=4, stride=2, padding=1),\n","            nn.ReLU(),\n","            SpatialAttention(512),  # Spatial Attention 추가\n","        )\n","\n","        # Decoder\n","        self.decoder = nn.Sequential(\n","            nn.ConvTranspose2d(512, 256, kernel_size=4, stride=2, padding=1),\n","            nn.ReLU(),\n","            nn.ConvTranspose2d(256, 128, kernel_size=4, stride=2, padding=1),\n","            nn.ReLU(),\n","            nn.ConvTranspose2d(128, 64, kernel_size=4, stride=2, padding=1),\n","            nn.ReLU(),\n","            nn.ConvTranspose2d(64, output_channels, kernel_size=4, stride=2, padding=1),\n","            nn.Tanh(),\n","        )\n","\n","    def forward(self, x):\n","        x = self.encoder(x)\n","        x = self.decoder(x)\n","        return x\n","\n","# Discriminator 모델 정의\n","class Discriminator(nn.Module):\n","    def __init__(self, input_channels):\n","        super(Discriminator, self).__init__()\n","\n","        self.model = nn.Sequential(\n","            nn.Conv2d(input_channels, 64, kernel_size=4, stride=2, padding=1),\n","            nn.LeakyReLU(0.2),\n","            nn.Conv2d(64, 128, kernel_size=4, stride=2, padding=1),\n","            nn.LeakyReLU(0.2),\n","            nn.Conv2d(128, 256, kernel_size=4, stride=2, padding=1),\n","            nn.LeakyReLU(0.2),\n","            nn.Conv2d(256, 512, kernel_size=4, stride=2, padding=1),\n","            nn.LeakyReLU(0.2),\n","            nn.Conv2d(512, 1, kernel_size=4, stride=1, padding=1),\n","            nn.Sigmoid(),\n","        )\n","\n","    def forward(self, x):\n","        return self.model(x)\n","```\n","\n","#### 5. 손실 함수 및 훈련 루프 정의\n","\n","```python\n","# Adversarial Loss (BCE)\n","def adversarial_loss(disc_pred, target):\n","    return torch.mean((disc_pred - target) ** 2)\n","\n","# Cycle Consistency Loss\n","def cycle_consistency_loss(real_image, reconstructed_image):\n","    return torch.mean((real_image - reconstructed_image) ** 2)\n","\n","# 훈련 루프\n","def train(cyclegan, dataloader, num_epochs, device):\n","    optimizer_g = torch.optim.Adam(cyclegan.parameters(), lr=0.0002, betas=(0.5, 0.999))\n","    optimizer_d = torch.optim.Adam(cyclegan.parameters(), lr=0.0002, betas=(0.5, 0.999))\n","\n","    for epoch in range(num_epochs):\n","        for i, data in enumerate(dataloader):\n","            real_A, real_B = data\n","            real_A, real_B = real_A.to(device), real_B.to(device)\n","\n","            # Generator A->B 업데이트 (스케치 -> 얼굴 사진)\n","            optimizer_g.zero_grad()\n","            fake_B = cyclegan.generator_AtoB(real_A)\n","            loss_G_AtoB = adversarial_loss(cyclegan.discriminator_B(fake_B), torch.ones_like(fake_B)) \\\n","                          + cycle_consistency_loss(real_A, cyclegan.generator_BtoA(fake_B))\n","            loss_G_AtoB.backward()\n","            optimizer_g.step()\n","\n","            # Generator B->A 업데이트 (얼굴 사진 -> 스케치)\n","            optimizer_g.zero_grad()\n","            fake_A = cyclegan.generator_BtoA(real_B)\n","            loss_G_BtoA = adversarial_loss(cyclegan.discriminator_A(fake_A), torch.ones_like(fake_A)) \\\n","                          + cycle_consistency_loss(real_B, cyclegan.generator_AtoB(fake_A))\n","            loss_G_BtoA.backward()\n","            optimizer_g.step()\n","\n","            # Discriminator A 업데이트\n","            optimizer_d.zero_grad()\n","            loss_D_A = adversarial_loss(cyclegan.discriminator_A(real_A), torch.ones_like(real_A)) \\\n","                       + adversarial_loss(cyclegan.discriminator_A(fake_A.detach()), torch.zeros_like(fake_A))\n","            loss_D_A.backward()\n","            optimizer_d.step()\n","\n","            # Discriminator B 업데이트\n","            optimizer_d.zero_grad()\n","            loss_D_B = adversarial_loss(cyclegan.discriminator_B(real_B), torch.ones_like(real_B)) \\\n","                       + adversarial_loss(cyclegan.discriminator_B(fake_B.detach()), torch.zeros_like(fake_B))\n","            loss_D_B.backward()\n","            optimizer_d.step()\n","\n","            if i % 100 == 0:\n","                print(f\"[Epoch {epoch}/{num_epochs}] [Batch {i}/{len(dataloader)}] \"\n","                      f\"[D loss: {loss_D_A.item() + loss_D_B.item()}] \"\n","                      f\"[G loss: {loss_G_AtoB.item() + loss_G_BtoA.item()}]\")\n","\n","        # 중간 결과 저장\n","        if epoch % 10 == 0:\n","            save_image(fake_B.data[:25], f\"generated_images_epoch_{epoch}.png\", nrow=5, normalize=True)\n","```\n","\n","#### 6. 모델 훈련 실행\n","\n","훈련을 시작합니다. 200번의 에폭 동안 훈련하며, 중간 결과를 저장합니다.\n","\n","```python\n","device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n","cyclegan = CycleGAN().to(device)  # CycleGAN 모델 인스턴스화\n","train(cyclegan, train_dataloader, num_epochs=200, device=device)\n","```\n","\n","이 코드를 Google Colab에서 실행하면, 스케치 이미지를 실제 얼굴 이미지로 변환하는 CycleGAN 모델이 훈련됩니다. 훈련 중에 결과 이미지를 주기적으로 저장하며, 200번의 에폭 동안 훈련이 이루어집니다.\n","\n","---\n","\n","### 주의사항\n","1. **훈련 데이터**는 스케치 이미지와 얼굴 이미지가 동일한 폴더 구조로 `filtered_images`에 존재해야 합니다.\n","2. **훈련 시간이 길 수 있습니다.** 200번의 에폭을 모두 훈련시키려면 시간이 오래 걸릴 수 있으니, 에폭 수를 조정하거나 Colab의 세션 제한을 확인하세요.\n","\n","이 코드로 원하는 작업을 수행할 수 있습니다!"]}],"metadata":{"accelerator":"GPU","colab":{"gpuType":"T4","machine_shape":"hm","provenance":[],"authorship_tag":"ABX9TyMlAJJzl2R+CJJQOgD4DPcY"},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"}},"nbformat":4,"nbformat_minor":0}